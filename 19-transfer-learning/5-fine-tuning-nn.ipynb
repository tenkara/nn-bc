{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "#transforms.RandomResizedCrop(224) --> A crop of random size (default: of 0.08 to 1.0) of the original size and a \n",
    "#random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. \n",
    "#This crop is finally resized to given size (224 in this case). \n",
    "#transforms.CenterCrop(224)--> Crops the image at the center. 224 is the Desired output size of the crop.\n",
    "#class torchvision.transforms.Normalize(mean, std)\n",
    "#Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, \n",
    "#this transform will normalize each channel of the input torch.Tensor i.e. \n",
    "#input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "#Parameters:     mean (sequence) – Sequence of means for each channel.\n",
    "#                std (sequence) – Sequence of standard deviations for each channel.\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), #Crop the given PIL Image to random size and aspect ratio.\n",
    "        transforms.RandomHorizontalFlip(), #Horizontally flip the given PIL Image randomly with a given probability.\n",
    "        transforms.ToTensor(), #Convert a PIL Image or numpy.ndarray to tensor.\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], #Normalize a tensor image with mean and standard deviation.\n",
    "                                [0.229, 0.224, 0.225]) #Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels,\n",
    "                                                        #this transform will normalize each channel of the input torch.Tensor\n",
    "                                                        #i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256), #Resize the input PIL Image to the given size.\n",
    "        transforms.CenterCrop(224), #Crops the image at the center. 224 is the Desired output size of the crop.\n",
    "        transforms.ToTensor(), #Convert a PIL Image or numpy.ndarray to tensor.\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], #Normalize a tensor image with mean and standard deviation.\n",
    "                                [0.229, 0.224, 0.225]) #Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels,\n",
    "                                                        #this transform will normalize each channel of the input torch.Tensor\n",
    "                                                        #i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['ants', 'bees']\n",
      "Dataset Sizes: {'train': 244, 'val': 153}\n",
      "Batches in training set: 61\n",
      "Batches in validation set: 39\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data/hymenoptera_data'\n",
    "# create a dictionary that contains the datasets for training and validation\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# create a dictionary that contains the dataloaders for training and validation\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "# create a dictionary that contains the size of the datasets for training and validation\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "# create a list that contains the names of the classes\n",
    "class_names = image_datasets['train'].classes\n",
    "# print the names and sizes of the datasets\n",
    "print(f\"Class Names: {class_names}\")\n",
    "print(f\"Dataset Sizes: {dataset_sizes}\")\n",
    "print(f\"Batches in training set: {len(dataloaders['train'])}\")\n",
    "print(f\"Batches in validation set: {len(dataloaders['val'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\nn-bc\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Raj\\repos\\nn-bc\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load the resnet18 model\n",
    "model_conv = models.resnet18(pretrained=True)\n",
    "# print the model\n",
    "print(model_conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the layers in the network\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of inputs of the last layer (or number of neurons in the layer preceeding the last layer)\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "# replace the last layer with a new layer that has 2 neurons (one for each class)\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_conv = model_conv.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Shape: torch.Size([4, 3, 224, 224])\n",
      "Labels: tensor([1, 0, 0, 0], device='cuda:0')\n",
      "Labels Shape: torch.Size([4])\n",
      "Output Tensor: tensor([[ 0.5429, -0.7551],\n",
      "        [-0.0869,  0.3366],\n",
      "        [ 0.5467, -0.3098],\n",
      "        [ 0.4152,  0.0230]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Output Shape: torch.Size([4, 2])\n",
      "Predicted: tensor([0, 1, 0, 0], device='cuda:0')\n",
      "Predicted Shape: torch.Size([4])\n",
      "Correct Predictions: 2\n"
     ]
    }
   ],
   "source": [
    "# Understand what is happening in the code below\n",
    "iteration = 0\n",
    "correct = 0\n",
    "for inputs, labels in dataloaders['train']:\n",
    "    if iteration == 1:\n",
    "        break\n",
    "    inputs = Variable(inputs.cuda())\n",
    "    labels = Variable(labels.cuda())\n",
    "    # print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Inputs Shape: {inputs.shape}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Labels Shape: {labels.shape}\")\n",
    "    output = model_conv(inputs)\n",
    "    print(f\"Output Tensor: {output}\")\n",
    "    print(f\"Output Shape: {output.shape}\")\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(f\"Predicted Shape: {predicted.shape}\")\n",
    "    correct += (predicted == labels).sum()\n",
    "    print(f\"Correct Predictions: {correct}\")\n",
    "\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "# try experimenting with different values of lr and momentum\n",
    "# decay lr by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand what is happening in the code above of scheduler.step()\n",
    "def lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print(f\"Learning Rate Changed to: {lr}\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\nn-bc\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Training Accuracy: 69.67212677001953%\n",
      "Epoch 2/25, Training Accuracy: 76.22950744628906%\n",
      "Epoch 3/25, Training Accuracy: 80.73770141601562%\n",
      "Epoch 4/25, Training Accuracy: 79.50819396972656%\n",
      "Epoch 5/25, Training Accuracy: 76.63934326171875%\n",
      "Epoch 6/25, Training Accuracy: 82.37704467773438%\n",
      "Epoch 7/25, Training Accuracy: 87.29507446289062%\n",
      "Epoch 8/25, Training Accuracy: 89.34425354003906%\n",
      "Epoch 9/25, Training Accuracy: 84.01638793945312%\n",
      "Epoch 10/25, Training Accuracy: 82.37704467773438%\n",
      "Epoch 11/25, Training Accuracy: 82.78688049316406%\n",
      "Epoch 12/25, Training Accuracy: 79.50819396972656%\n",
      "Epoch 13/25, Training Accuracy: 85.24589538574219%\n",
      "Epoch 14/25, Training Accuracy: 87.70491027832031%\n",
      "Epoch 15/25, Training Accuracy: 87.70491027832031%\n",
      "Epoch 16/25, Training Accuracy: 81.557373046875%\n",
      "Epoch 17/25, Training Accuracy: 84.42622375488281%\n",
      "Epoch 18/25, Training Accuracy: 83.60655212402344%\n",
      "Epoch 19/25, Training Accuracy: 84.01638793945312%\n",
      "Epoch 20/25, Training Accuracy: 86.88523864746094%\n",
      "Epoch 21/25, Training Accuracy: 89.34425354003906%\n",
      "Epoch 22/25, Training Accuracy: 81.96720886230469%\n",
      "Epoch 23/25, Training Accuracy: 86.88523864746094%\n",
      "Epoch 24/25, Training Accuracy: 83.60655212402344%\n",
      "Epoch 25/25, Training Accuracy: 86.88523864746094%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    exp_lr_scheduler.step()\n",
    "    # reset the correct to 0 at the beginning of each epoch\n",
    "    correct = 0\n",
    "    for images, labels in dataloaders['train']:\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model_conv(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backpropagation: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # get the predicted class from the maximum value in the output-list of class scores\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # calculate the accuracy\n",
    "        correct += (predicted == labels.data).sum()\n",
    "    train_acc = 100 * correct / dataset_sizes['train']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {train_acc}%\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
