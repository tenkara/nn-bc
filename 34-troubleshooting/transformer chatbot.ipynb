{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = 'data/cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "corpus_movie_lines = 'data/cornell movie-dialogs corpus/movie_lines.txt'\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    conv = c.readlines()\n",
    "\n",
    "# conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_lines, 'r') as l:\n",
    "    lines = l.readlines()\n",
    "\n",
    "# lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dic = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dic[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char  # space is also a character\n",
    "    return no_punct.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they do not\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ele = remove_punc(lines_dic['L1045'])\n",
    "ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        \n",
    "        if i==len(ids)-1:\n",
    "            break\n",
    "        \n",
    "        first = remove_punc(lines_dic[ids[i]].strip())      \n",
    "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        pairs.append(qa_pairs)\n",
    "\n",
    "# pairs\n",
    "# len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'can': 14103,\n",
       "         'we': 25912,\n",
       "         'make': 5821,\n",
       "         'this': 30502,\n",
       "         'quick': 310,\n",
       "         'roxanne': 1,\n",
       "         'korrine': 1,\n",
       "         'and': 52128,\n",
       "         'andrew': 49,\n",
       "         'barrett': 20,\n",
       "         'are': 21713,\n",
       "         'having': 1081,\n",
       "         'an': 8827,\n",
       "         'incredibly': 49,\n",
       "         'horrendous': 4,\n",
       "         'public': 306,\n",
       "         'break': 799,\n",
       "         'up': 14316,\n",
       "         'on': 23908,\n",
       "         'the': 120903,\n",
       "         'quad': 2,\n",
       "         'again': 2807,\n",
       "         'well': 16263,\n",
       "         'i': 137633,\n",
       "         'thought': 4202,\n",
       "         'wed': 541,\n",
       "         'start': 1459,\n",
       "         'with': 21394,\n",
       "         'pronunciation': 2,\n",
       "         'if': 16727,\n",
       "         'thats': 14742,\n",
       "         'okay': 5946,\n",
       "         'you': 169693,\n",
       "         'not': 26494,\n",
       "         'hacking': 18,\n",
       "         'gagging': 9,\n",
       "         'spitting': 15,\n",
       "         'part': 1260,\n",
       "         'please': 3258,\n",
       "         'then': 7532,\n",
       "         'how': 14001,\n",
       "         'bout': 393,\n",
       "         'try': 1998,\n",
       "         'out': 16100,\n",
       "         'some': 8153,\n",
       "         'french': 306,\n",
       "         'cuisine': 11,\n",
       "         'saturday': 184,\n",
       "         'night': 3489,\n",
       "         'youre': 18178,\n",
       "         'asking': 691,\n",
       "         'me': 41056,\n",
       "         'so': 16699,\n",
       "         'cute': 259,\n",
       "         'whats': 6573,\n",
       "         'your': 26790,\n",
       "         'name': 2839,\n",
       "         'forget': 1316,\n",
       "         'it': 60008,\n",
       "         'no': 27118,\n",
       "         'its': 24458,\n",
       "         'my': 26249,\n",
       "         'fault': 456,\n",
       "         'didnt': 8043,\n",
       "         'have': 27980,\n",
       "         'a': 89106,\n",
       "         'proper': 126,\n",
       "         'introduction': 19,\n",
       "         'cameron': 34,\n",
       "         'thing': 4976,\n",
       "         'is': 37369,\n",
       "         'im': 30345,\n",
       "         'at': 13283,\n",
       "         'mercy': 62,\n",
       "         'of': 47394,\n",
       "         'particularly': 102,\n",
       "         'hideous': 19,\n",
       "         'breed': 31,\n",
       "         'loser': 113,\n",
       "         'sister': 585,\n",
       "         'cant': 8757,\n",
       "         'date': 507,\n",
       "         'until': 1236,\n",
       "         'she': 11344,\n",
       "         'does': 3336,\n",
       "         'seems': 751,\n",
       "         'like': 19130,\n",
       "         'could': 7311,\n",
       "         'get': 17562,\n",
       "         'easy': 1089,\n",
       "         'enough': 2295,\n",
       "         'why': 12219,\n",
       "         'unsolved': 13,\n",
       "         'mystery': 84,\n",
       "         'used': 1767,\n",
       "         'to': 100667,\n",
       "         'be': 24192,\n",
       "         'really': 6196,\n",
       "         'popular': 94,\n",
       "         'when': 9007,\n",
       "         'started': 709,\n",
       "         'high': 749,\n",
       "         'school': 1358,\n",
       "         'was': 25287,\n",
       "         'just': 20634,\n",
       "         'got': 14743,\n",
       "         'sick': 852,\n",
       "         'or': 7776,\n",
       "         'something': 6827,\n",
       "         'shame': 149,\n",
       "         'gosh': 103,\n",
       "         'only': 5044,\n",
       "         'find': 3549,\n",
       "         'kat': 39,\n",
       "         'boyfriend': 253,\n",
       "         'let': 4589,\n",
       "         'see': 10290,\n",
       "         'what': 44770,\n",
       "         'do': 30486,\n",
       "         'cesc': 1,\n",
       "         'ma': 309,\n",
       "         'tete': 1,\n",
       "         'head': 1438,\n",
       "         'right': 12791,\n",
       "         'ready': 1076,\n",
       "         'for': 29046,\n",
       "         'quiz': 9,\n",
       "         'dont': 33134,\n",
       "         'want': 14744,\n",
       "         'know': 29118,\n",
       "         'say': 8124,\n",
       "         'that': 44755,\n",
       "         'though': 842,\n",
       "         'useful': 66,\n",
       "         'things': 3447,\n",
       "         'where': 7503,\n",
       "         'good': 9378,\n",
       "         'stores': 35,\n",
       "         'much': 4715,\n",
       "         'because': 4218,\n",
       "         'such': 1326,\n",
       "         'nice': 2319,\n",
       "         'one': 12865,\n",
       "         'our': 4763,\n",
       "         'little': 5569,\n",
       "         'wench': 10,\n",
       "         'plan': 611,\n",
       "         'progressing': 3,\n",
       "         'theres': 5085,\n",
       "         'someone': 2121,\n",
       "         'think': 14340,\n",
       "         'might': 2406,\n",
       "         'there': 12922,\n",
       "         'mind': 2249,\n",
       "         'counted': 31,\n",
       "         'help': 3299,\n",
       "         'cause': 1230,\n",
       "         'thug': 7,\n",
       "         'obviously': 224,\n",
       "         'failing': 19,\n",
       "         'arent': 1347,\n",
       "         'ever': 3650,\n",
       "         'going': 11379,\n",
       "         'word': 1164,\n",
       "         'as': 9331,\n",
       "         'gentleman': 153,\n",
       "         'sweet': 442,\n",
       "         'hair': 532,\n",
       "         'look': 7492,\n",
       "         'ebers': 1,\n",
       "         'deep': 314,\n",
       "         'conditioner': 11,\n",
       "         'every': 2226,\n",
       "         'two': 4538,\n",
       "         'days': 1424,\n",
       "         'never': 7067,\n",
       "         'use': 1444,\n",
       "         'blowdryer': 2,\n",
       "         'without': 1488,\n",
       "         'diffuser': 1,\n",
       "         'attachment': 5,\n",
       "         'sure': 6047,\n",
       "         'wanna': 1241,\n",
       "         'go': 12495,\n",
       "         'but': 22020,\n",
       "         'unless': 479,\n",
       "         'goes': 873,\n",
       "         'workin': 92,\n",
       "         'doesnt': 3054,\n",
       "         'seem': 738,\n",
       "         'goin': 604,\n",
       "         'him': 14719,\n",
       "         'shes': 3954,\n",
       "         'lesbian': 28,\n",
       "         'found': 1550,\n",
       "         'picture': 549,\n",
       "         'jared': 3,\n",
       "         'leto': 6,\n",
       "         'in': 41691,\n",
       "         'her': 11430,\n",
       "         'drawers': 15,\n",
       "         'pretty': 1791,\n",
       "         'harboring': 6,\n",
       "         'samesex': 3,\n",
       "         'tendencies': 6,\n",
       "         'kind': 2730,\n",
       "         'guy': 3124,\n",
       "         'likes': 369,\n",
       "         'ones': 702,\n",
       "         'who': 9152,\n",
       "         'knows': 1204,\n",
       "         'all': 18850,\n",
       "         'ive': 7039,\n",
       "         'heard': 1976,\n",
       "         'shed': 281,\n",
       "         'dip': 24,\n",
       "         'before': 3587,\n",
       "         'dating': 86,\n",
       "         'smokes': 38,\n",
       "         'hi': 1204,\n",
       "         'looks': 1232,\n",
       "         'worked': 569,\n",
       "         'tonight': 1819,\n",
       "         'huh': 2286,\n",
       "         'chastity': 12,\n",
       "         'believe': 3360,\n",
       "         'share': 248,\n",
       "         'art': 290,\n",
       "         'instructor': 6,\n",
       "         'fun': 658,\n",
       "         'tons': 51,\n",
       "         'looked': 555,\n",
       "         'back': 7418,\n",
       "         'party': 809,\n",
       "         'always': 3299,\n",
       "         'seemed': 238,\n",
       "         'occupied': 25,\n",
       "         'wanted': 2354,\n",
       "         'did': 11887,\n",
       "         'had': 7015,\n",
       "         'been': 8487,\n",
       "         'selfish': 56,\n",
       "         'guillermo': 1,\n",
       "         'says': 1567,\n",
       "         'any': 5405,\n",
       "         'lighter': 28,\n",
       "         'gonna': 5323,\n",
       "         'extra': 200,\n",
       "         '90210': 1,\n",
       "         'listen': 2434,\n",
       "         'crap': 273,\n",
       "         'endless': 20,\n",
       "         'blonde': 105,\n",
       "         'babble': 7,\n",
       "         'boring': 144,\n",
       "         'myself': 1445,\n",
       "         'thank': 2557,\n",
       "         'god': 2881,\n",
       "         'hear': 2351,\n",
       "         'more': 5498,\n",
       "         'story': 1143,\n",
       "         'about': 18683,\n",
       "         'coiffure': 4,\n",
       "         'figured': 438,\n",
       "         'youd': 1980,\n",
       "         'stuff': 1427,\n",
       "         'eventually': 103,\n",
       "         'real': 2247,\n",
       "         'fear': 302,\n",
       "         'wearing': 334,\n",
       "         'pastels': 2,\n",
       "         'kidding': 587,\n",
       "         'sometimes': 934,\n",
       "         'become': 398,\n",
       "         'persona': 4,\n",
       "         'quit': 461,\n",
       "         'need': 5194,\n",
       "         'learn': 541,\n",
       "         'lie': 600,\n",
       "         'wow': 348,\n",
       "         'lets': 3125,\n",
       "         'hope': 1256,\n",
       "         'they': 15365,\n",
       "         'change': 966,\n",
       "         'he': 24379,\n",
       "         'here': 15848,\n",
       "         'joey': 164,\n",
       "         'great': 2823,\n",
       "         'would': 7915,\n",
       "         'getting': 2127,\n",
       "         'drink': 1073,\n",
       "         'practically': 131,\n",
       "         'proposed': 44,\n",
       "         'same': 1858,\n",
       "         'dermatologist': 3,\n",
       "         'mean': 7182,\n",
       "         'dr': 1071,\n",
       "         'bonchowski': 1,\n",
       "         'hes': 8639,\n",
       "         'exactly': 1380,\n",
       "         'relevant': 23,\n",
       "         'oily': 7,\n",
       "         'dry': 168,\n",
       "         'combination': 58,\n",
       "         'hed': 532,\n",
       "         'different': 1030,\n",
       "         'bianca': 31,\n",
       "         'highlights': 6,\n",
       "         'dorsey': 8,\n",
       "         'include': 53,\n",
       "         'dooropening': 1,\n",
       "         'coatholding': 1,\n",
       "         'wonder': 505,\n",
       "         'guys': 2071,\n",
       "         'were': 14086,\n",
       "         'supposed': 1253,\n",
       "         'actually': 1166,\n",
       "         'id': 3964,\n",
       "         'give': 4358,\n",
       "         'private': 399,\n",
       "         'line': 678,\n",
       "         'home': 2769,\n",
       "         'twenty': 742,\n",
       "         'minutes': 1168,\n",
       "         'til': 167,\n",
       "         're': 64,\n",
       "         'sophomore': 13,\n",
       "         'prom': 101,\n",
       "         'expensive': 110,\n",
       "         'bogey': 11,\n",
       "         'lowenbraus': 1,\n",
       "         'hopefully': 28,\n",
       "         'yeah': 10448,\n",
       "         'sears': 9,\n",
       "         'catalog': 5,\n",
       "         'tube': 37,\n",
       "         'sock': 19,\n",
       "         'gig': 59,\n",
       "         'huge': 126,\n",
       "         'ad': 66,\n",
       "         'queen': 179,\n",
       "         'harry': 596,\n",
       "         'gay': 118,\n",
       "         'cruise': 30,\n",
       "         'ill': 8849,\n",
       "         'uniform': 85,\n",
       "         'neat': 70,\n",
       "         'agent': 391,\n",
       "         'shot': 993,\n",
       "         'being': 1941,\n",
       "         'prada': 4,\n",
       "         'next': 1543,\n",
       "         'year': 1055,\n",
       "         'hey': 3185,\n",
       "         'cheeks': 23,\n",
       "         'concentrating': 16,\n",
       "         'awfully': 71,\n",
       "         'hard': 1384,\n",
       "         'considering': 87,\n",
       "         'gym': 27,\n",
       "         'class': 413,\n",
       "         'talk': 4268,\n",
       "         'deal': 1253,\n",
       "         't': 95,\n",
       "         'whereve': 29,\n",
       "         'nowhere': 175,\n",
       "         'daddy': 696,\n",
       "         'potential': 59,\n",
       "         'smack': 21,\n",
       "         'way': 6112,\n",
       "         'least': 1088,\n",
       "         'bra': 28,\n",
       "         'oh': 10883,\n",
       "         'becoming': 83,\n",
       "         'normal': 256,\n",
       "         'means': 850,\n",
       "         'gigglepuss': 6,\n",
       "         'playing': 546,\n",
       "         'club': 307,\n",
       "         'skunk': 15,\n",
       "         'bothering': 92,\n",
       "         'ask': 2462,\n",
       "         'lowensteins': 3,\n",
       "         'freak': 142,\n",
       "         'torture': 71,\n",
       "         'suck': 108,\n",
       "         'ruining': 35,\n",
       "         'life': 3326,\n",
       "         'wont': 3337,\n",
       "         'too': 6295,\n",
       "         'busy': 449,\n",
       "         'listening': 311,\n",
       "         'bitches': 46,\n",
       "         'prozac': 6,\n",
       "         'completely': 372,\n",
       "         'wretched': 24,\n",
       "         'clouted': 1,\n",
       "         'fen': 1,\n",
       "         'sucked': 44,\n",
       "         'hedgepig': 1,\n",
       "         'even': 3907,\n",
       "         'shakespeare': 53,\n",
       "         'maybe': 5188,\n",
       "         'youve': 3787,\n",
       "         'friend': 1826,\n",
       "         'mandellas': 1,\n",
       "         'guess': 2645,\n",
       "         'since': 1437,\n",
       "         'allowed': 153,\n",
       "         'should': 4563,\n",
       "         'obsess': 1,\n",
       "         'over': 4909,\n",
       "         'dead': 2470,\n",
       "         'unbalanced': 2,\n",
       "         'now': 12037,\n",
       "         'tell': 8991,\n",
       "         'social': 127,\n",
       "         'advice': 235,\n",
       "         'from': 8496,\n",
       "         'act': 503,\n",
       "         'totally': 322,\n",
       "         'apeshit': 3,\n",
       "         'welcome': 341,\n",
       "         'hate': 1041,\n",
       "         'sit': 1044,\n",
       "         'susie': 79,\n",
       "         'care': 2058,\n",
       "         'firm': 109,\n",
       "         'believer': 21,\n",
       "         'doing': 4013,\n",
       "         'own': 1961,\n",
       "         'reasons': 172,\n",
       "         'else': 2336,\n",
       "         's': 97,\n",
       "         'wish': 1030,\n",
       "         'luxury': 19,\n",
       "         'asked': 1091,\n",
       "         'won': 261,\n",
       "         'told': 3710,\n",
       "         'went': 1891,\n",
       "         '9th': 14,\n",
       "         'month': 403,\n",
       "         'total': 165,\n",
       "         'babe': 146,\n",
       "         'said': 5404,\n",
       "         'everyone': 811,\n",
       "         'once': 1540,\n",
       "         'afterwards': 50,\n",
       "         'anymore': 906,\n",
       "         'wasnt': 2242,\n",
       "         'pissed': 151,\n",
       "         'broke': 405,\n",
       "         'after': 3015,\n",
       "         'swore': 62,\n",
       "         'anything': 4396,\n",
       "         'havent': 1917,\n",
       "         'except': 531,\n",
       "         'bogeys': 3,\n",
       "         'decisions': 48,\n",
       "         'wouldve': 190,\n",
       "         'instead': 320,\n",
       "         'helping': 169,\n",
       "         'stupid': 768,\n",
       "         'repeat': 123,\n",
       "         'mistakes': 80,\n",
       "         'protecting': 81,\n",
       "         'keep': 2831,\n",
       "         'locked': 224,\n",
       "         'away': 2955,\n",
       "         'dark': 471,\n",
       "         'experience': 273,\n",
       "         'experiences': 33,\n",
       "         'trust': 967,\n",
       "         'people': 5027,\n",
       "         'will': 7900,\n",
       "         'beautiful': 1043,\n",
       "         'last': 3386,\n",
       "         'set': 921,\n",
       "         'damage': 148,\n",
       "         'send': 800,\n",
       "         'therapy': 86,\n",
       "         'forever': 263,\n",
       "         'woman': 1418,\n",
       "         'complete': 190,\n",
       "         'fruitloop': 1,\n",
       "         'patrick': 119,\n",
       "         'perm': 8,\n",
       "         'upset': 367,\n",
       "         'boy': 1874,\n",
       "         'starts': 160,\n",
       "         'end': 969,\n",
       "         'discussion': 64,\n",
       "         'neither': 346,\n",
       "         'sleep': 1090,\n",
       "         'fair': 404,\n",
       "         'mutant': 27,\n",
       "         'point': 1305,\n",
       "         'wherere': 43,\n",
       "         'must': 3306,\n",
       "         'attempting': 21,\n",
       "         'small': 559,\n",
       "         'study': 215,\n",
       "         'group': 238,\n",
       "         'friends': 1322,\n",
       "         'otherwise': 187,\n",
       "         'known': 545,\n",
       "         'orgy': 11,\n",
       "         'knew': 1951,\n",
       "         'forbid': 21,\n",
       "         'gloria': 30,\n",
       "         'steinem': 3,\n",
       "         'isnt': 3129,\n",
       "         'expect': 571,\n",
       "         'kats': 1,\n",
       "         'starting': 286,\n",
       "         'wear': 443,\n",
       "         'belly': 46,\n",
       "         'minute': 1346,\n",
       "         'promise': 638,\n",
       "         'boys': 743,\n",
       "         'present': 292,\n",
       "         'shell': 495,\n",
       "         'scare': 185,\n",
       "         'them': 7208,\n",
       "         'discuss': 213,\n",
       "         'tomorrow': 1583,\n",
       "         'has': 4569,\n",
       "         'hot': 632,\n",
       "         'rod': 64,\n",
       "         'bend': 53,\n",
       "         'rules': 240,\n",
       "         'whatever': 1066,\n",
       "         'fine': 2471,\n",
       "         'prisoner': 67,\n",
       "         'house': 2089,\n",
       "         'daughter': 545,\n",
       "         'possession': 79,\n",
       "         'missing': 323,\n",
       "         'captain': 862,\n",
       "         'oppression': 7,\n",
       "         'men': 1467,\n",
       "         'pleasure': 324,\n",
       "         'brucie': 1,\n",
       "         'pegged': 19,\n",
       "         'fan': 136,\n",
       "         'preteen': 3,\n",
       "         'bellybutton': 3,\n",
       "         'ring': 323,\n",
       "         'couple': 981,\n",
       "         'minors': 12,\n",
       "         'come': 8509,\n",
       "         'padua': 2,\n",
       "         'girls': 720,\n",
       "         'tall': 109,\n",
       "         'decent': 116,\n",
       "         'body': 720,\n",
       "         'other': 3340,\n",
       "         'kinda': 419,\n",
       "         'short': 318,\n",
       "         'undersexed': 2,\n",
       "         'sent': 673,\n",
       "         'em': 1620,\n",
       "         'through': 2100,\n",
       "         'new': 2741,\n",
       "         'cmon': 536,\n",
       "         'tour': 93,\n",
       "         'which': 1660,\n",
       "         'dakota': 20,\n",
       "         'north': 222,\n",
       "         'howd': 488,\n",
       "         'live': 1690,\n",
       "         'outnumbered': 6,\n",
       "         'by': 4866,\n",
       "         'cows': 36,\n",
       "         'many': 1648,\n",
       "         'old': 2969,\n",
       "         'thirtytwo': 22,\n",
       "         'thousand': 1057,\n",
       "         'most': 1452,\n",
       "         'evil': 307,\n",
       "         'these': 3406,\n",
       "         'seen': 1863,\n",
       "         'horse': 271,\n",
       "         'jack': 1026,\n",
       "         'off': 4779,\n",
       "         'clint': 8,\n",
       "         'eastwood': 6,\n",
       "         'girl': 2391,\n",
       "         'burn': 172,\n",
       "         'pine': 20,\n",
       "         'perish': 10,\n",
       "         'stratford': 10,\n",
       "         'haircut': 39,\n",
       "         'matter': 1675,\n",
       "         'older': 176,\n",
       "         'impossibility': 4,\n",
       "         'theyre': 3890,\n",
       "         'bred': 17,\n",
       "         'their': 2501,\n",
       "         'mothers': 284,\n",
       "         'liked': 424,\n",
       "         'grandmothers': 18,\n",
       "         'gene': 35,\n",
       "         'pool': 177,\n",
       "         'rarely': 31,\n",
       "         'diluted': 2,\n",
       "         'shiteating': 3,\n",
       "         'grin': 4,\n",
       "         'permashitgrin': 1,\n",
       "         'moron': 68,\n",
       "         'number': 826,\n",
       "         'twelve': 384,\n",
       "         'model': 94,\n",
       "         'mostly': 193,\n",
       "         'regional': 14,\n",
       "         'moms': 105,\n",
       "         'canada': 43,\n",
       "         'signed': 128,\n",
       "         'tutor': 13,\n",
       "         'chance': 984,\n",
       "         'consecrate': 1,\n",
       "         'minor': 73,\n",
       "         'encounter': 22,\n",
       "         'shrew': 1,\n",
       "         'biancas': 3,\n",
       "         'mewling': 1,\n",
       "         'rampalian': 1,\n",
       "         'wretch': 5,\n",
       "         'herself': 213,\n",
       "         'teach': 272,\n",
       "         'dazzle': 3,\n",
       "         'charm': 54,\n",
       "         'falls': 106,\n",
       "         'love': 3993,\n",
       "         'unlikely': 27,\n",
       "         'still': 3648,\n",
       "         'makes': 1209,\n",
       "         'hell': 3710,\n",
       "         'thrives': 2,\n",
       "         'danger': 207,\n",
       "         'criminal': 169,\n",
       "         'lit': 37,\n",
       "         'state': 477,\n",
       "         'trooper': 17,\n",
       "         'fire': 742,\n",
       "         'alcatraz': 2,\n",
       "         'felons': 3,\n",
       "         'honors': 18,\n",
       "         'biology': 12,\n",
       "         'serious': 785,\n",
       "         'man': 6906,\n",
       "         'whacked': 31,\n",
       "         'sold': 220,\n",
       "         'his': 8551,\n",
       "         'liver': 25,\n",
       "         'black': 737,\n",
       "         'market': 145,\n",
       "         'buy': 827,\n",
       "         'speakers': 6,\n",
       "         'reputation': 115,\n",
       "         'weve': 1785,\n",
       "         'outrank': 2,\n",
       "         'strictly': 51,\n",
       "         'alist': 2,\n",
       "         'side': 827,\n",
       "         'hated': 162,\n",
       "         'those': 3210,\n",
       "         'gotta': 2246,\n",
       "         'few': 1451,\n",
       "         'clients': 84,\n",
       "         'wall': 249,\n",
       "         'street': 673,\n",
       "         'involved': 378,\n",
       "         'choice': 452,\n",
       "         'besides': 515,\n",
       "         'enemy': 187,\n",
       "         'orchestrating': 1,\n",
       "         'battle': 110,\n",
       "         'position': 261,\n",
       "         'power': 676,\n",
       "         'golden': 65,\n",
       "         'opportunity': 143,\n",
       "         'katarina': 3,\n",
       "         'case': 1278,\n",
       "         'schoolwide': 2,\n",
       "         'blow': 435,\n",
       "         'bent': 48,\n",
       "         'piss': 125,\n",
       "         'himself': 645,\n",
       "         'joy': 66,\n",
       "         'ultimate': 39,\n",
       "         'kiss': 364,\n",
       "         'ass': 1043,\n",
       "         'hates': 137,\n",
       "         'smokers': 4,\n",
       "         'lung': 33,\n",
       "         'cancer': 134,\n",
       "         'issue': 139,\n",
       "         'favorite': 234,\n",
       "         'uncle': 360,\n",
       "         'fortyone': 11,\n",
       "         'assail': 1,\n",
       "         'ears': 150,\n",
       "         'band': 171,\n",
       "         'already': 1511,\n",
       "         'whole': 1689,\n",
       "         'extremely': 102,\n",
       "         'unfortunate': 62,\n",
       "         'maneuver': 17,\n",
       "         'picks': 37,\n",
       "         'carries': 22,\n",
       "         'while': 1445,\n",
       "         'talking': 2690,\n",
       "         'buttholus': 2,\n",
       "         'extremus': 2,\n",
       "         'making': 960,\n",
       "         'progress': 96,\n",
       "         'm': 96,\n",
       "         'humiliated': 24,\n",
       "         'sacrifice': 54,\n",
       "         'yourself': 2210,\n",
       "         'altar': 10,\n",
       "         'dignity': 38,\n",
       "         'score': 116,\n",
       "         'best': 1669,\n",
       "         'scenario': 21,\n",
       "         'payroll': 30,\n",
       "         'awhile': 169,\n",
       "         'non': 36,\n",
       "         'prisonmovie': 1,\n",
       "         'type': 267,\n",
       "         'whatve': 25,\n",
       "         'retrieved': 4,\n",
       "         'certain': 395,\n",
       "         'pieces': 147,\n",
       "         'information': 469,\n",
       "         'miss': 1691,\n",
       "         'youll': 2796,\n",
       "         'helpful': 53,\n",
       "         'thai': 10,\n",
       "         'food': 536,\n",
       "         'feminist': 7,\n",
       "         'prose': 6,\n",
       "         'angry': 251,\n",
       "         'stinky': 10,\n",
       "         'music': 438,\n",
       "         'indierock': 1,\n",
       "         'persuasion': 4,\n",
       "         'noodles': 7,\n",
       "         'book': 754,\n",
       "         'around': 3391,\n",
       "         'chicks': 73,\n",
       "         'play': 1423,\n",
       "         'partial': 29,\n",
       "         'whatd': 471,\n",
       "         'don': 198,\n",
       "         'decided': 325,\n",
       "         'nail': 91,\n",
       "         'drunk': 384,\n",
       "         'remember': 2678,\n",
       "         'suns': 25,\n",
       "         'direct': 123,\n",
       "         'quote': 63,\n",
       "         'needs': 606,\n",
       "         'time': 8172,\n",
       "         'cool': 625,\n",
       "         'day': 2880,\n",
       "         'makin': 91,\n",
       "         'headway': 3,\n",
       "         'kissed': 76,\n",
       "         'worst': 227,\n",
       "         'vintage': 8,\n",
       "         'reading': 285,\n",
       "         'sassy': 11,\n",
       "         'noticed': 208,\n",
       "         'featured': 3,\n",
       "         'big': 2744,\n",
       "         'kmart': 7,\n",
       "         'spread': 74,\n",
       "         'elbow': 13,\n",
       "         'tough': 398,\n",
       "         'running': 758,\n",
       "         'rest': 905,\n",
       "         'ya': 1678,\n",
       "         'leave': 2485,\n",
       "         'alone': 1362,\n",
       "         'legs': 185,\n",
       "         'rack': 23,\n",
       "         'sparky': 10,\n",
       "         'money': 3437,\n",
       "         'take': 7002,\n",
       "         'cake': 107,\n",
       "         'verona': 19,\n",
       "         'pick': 779,\n",
       "         'tab': 27,\n",
       "         'pay': 1074,\n",
       "         'gets': 993,\n",
       "         'catch': 516,\n",
       "         'bucks': 397,\n",
       "         'thirty': 463,\n",
       "         'negotiation': 16,\n",
       "         'fifty': 569,\n",
       "         'results': 56,\n",
       "         'watching': 402,\n",
       "         'bitch': 504,\n",
       "         'trash': 61,\n",
       "         'car': 1948,\n",
       "         'count': 350,\n",
       "         'under': 1068,\n",
       "         'control': 522,\n",
       "         'acts': 41,\n",
       "         'crazed': 8,\n",
       "         'image': 91,\n",
       "         'upped': 9,\n",
       "         'price': 285,\n",
       "         'hundred': 1392,\n",
       "         'deals': 82,\n",
       "         'human': 509,\n",
       "         'limothe': 1,\n",
       "         'flowers': 155,\n",
       "         'another': 2060,\n",
       "         'tux': 6,\n",
       "         'barbie': 14,\n",
       "         'n': 109,\n",
       "         'ken': 36,\n",
       "         'shit': 2940,\n",
       "         'lost': 1133,\n",
       "         'nope': 229,\n",
       "         'came': 2074,\n",
       "         'chat': 82,\n",
       "         'run': 1496,\n",
       "         'idea': 1708,\n",
       "         'interested': 521,\n",
       "         'insane': 222,\n",
       "         'conversation': 192,\n",
       "         'purpose': 162,\n",
       "         'recruit': 15,\n",
       "         'wholl': 41,\n",
       "         'whos': 1478,\n",
       "         'job': 1985,\n",
       "         'helpin': 9,\n",
       "         'uh': 2229,\n",
       "         'took': 1378,\n",
       "         'bathes': 3,\n",
       "         'together': 1390,\n",
       "         'kids': 1143,\n",
       "         'better': 3434,\n",
       "         'fuck': 2598,\n",
       "         'heavily': 24,\n",
       "         'invested': 19,\n",
       "         'higher': 91,\n",
       "         'random': 56,\n",
       "         'skid': 8,\n",
       "         'pat': 56,\n",
       "         'gone': 1303,\n",
       "         'porn': 25,\n",
       "         'movies': 309,\n",
       "         'incapable': 24,\n",
       "         'interesting': 417,\n",
       "         'block': 127,\n",
       "         'e': 63,\n",
       "         'mandella': 1,\n",
       "         'eat': 954,\n",
       "         'starving': 49,\n",
       "         'very': 5095,\n",
       "         'slow': 315,\n",
       "         'die': 1170,\n",
       "         'attempted': 22,\n",
       "         'slit': 27,\n",
       "         'realize': 369,\n",
       "         'institution': 57,\n",
       "         'severely': 10,\n",
       "         'lacking': 23,\n",
       "         'killing': 380,\n",
       "         'william': 137,\n",
       "         'beyond': 196,\n",
       "         'imagine': 405,\n",
       "         'during': 252,\n",
       "         'sex': 602,\n",
       "         'foul': 38,\n",
       "         'raging': 8,\n",
       "         'fit': 247,\n",
       "         'sarah': 121,\n",
       "         'lawrence': 19,\n",
       "         'insists': 17,\n",
       "         'maledominated': 1,\n",
       "         'puking': 7,\n",
       "         'frat': 9,\n",
       "         'golf': 67,\n",
       "         'team': 327,\n",
       "         'proven': 33,\n",
       "         'heterosexuality': 2,\n",
       "         'appreciate': 353,\n",
       "         'efforts': 25,\n",
       "         'toward': 114,\n",
       "         'speedy': 3,\n",
       "         'death': 928,\n",
       "         'consuming': 7,\n",
       "         'precious': 71,\n",
       "         'tiara': 2,\n",
       "         'thisll': 54,\n",
       "         'work': 3572,\n",
       "         'cares': 173,\n",
       "         'officially': 45,\n",
       "         'opposed': 41,\n",
       "         'suburban': 8,\n",
       "         'activity': 42,\n",
       "         'wheres': 1178,\n",
       "         'done': 2170,\n",
       "         'favor': 402,\n",
       "         'backfired': 2,\n",
       "         'puked': 3,\n",
       "         'rejected': 18,\n",
       "         'bastion': 4,\n",
       "         'commercial': 57,\n",
       "         'excess': 8,\n",
       "         'dates': 67,\n",
       "         'sound': 643,\n",
       "         'betty': 244,\n",
       "         'archie': 24,\n",
       "         'taking': 1002,\n",
       "         'veronica': 87,\n",
       "         'dress': 309,\n",
       "         'anyway': 1300,\n",
       "         'looking': 1803,\n",
       "         'wrong': 2488,\n",
       "         'perspective': 31,\n",
       "         'statement': 119,\n",
       "         'us': 7005,\n",
       "         'meet': 1365,\n",
       "         'honey': 909,\n",
       "         'progressed': 3,\n",
       "         'fullon': 4,\n",
       "         'hallucinations': 15,\n",
       "         'doin': 489,\n",
       "         'sweating': 32,\n",
       "         'pig': 156,\n",
       "         'attention': 267,\n",
       "         'mission': 254,\n",
       "         'friday': 168,\n",
       "         'places': 237,\n",
       "         '7eleven': 4,\n",
       "         'burnside': 2,\n",
       "         'screwboy': 2,\n",
       "         'lot': 2521,\n",
       "         'than': 3281,\n",
       "         'warrant': 80,\n",
       "         'strong': 312,\n",
       "         'emotion': 35,\n",
       "         'spend': 343,\n",
       "         'dollar': 236,\n",
       "         'track': 192,\n",
       "         'ponies': 18,\n",
       "         'flat': 121,\n",
       "         'beer': 296,\n",
       "         'eyes': 828,\n",
       "         'hand': 891,\n",
       "         'covered': 124,\n",
       "         'vomit': 35,\n",
       "         'seventhirty': 12,\n",
       "         'following': 203,\n",
       "         'laundromat': 8,\n",
       "         'saw': 2020,\n",
       "         'talker': 10,\n",
       "         'depends': 183,\n",
       "         'topic': 22,\n",
       "         'fenders': 2,\n",
       "         'whip': 41,\n",
       "         'into': 3618,\n",
       "         'verbal': 25,\n",
       "         'frenzy': 6,\n",
       "         'show': 1723,\n",
       "         'excuse': 845,\n",
       "         'sort': 825,\n",
       "         'bikini': 10,\n",
       "         'kill': 2499,\n",
       "         'raincoats': 4,\n",
       "         'trashed': 14,\n",
       "         'funny': 933,\n",
       "         'down': 5401,\n",
       "         'concussion': 15,\n",
       "         'dog': 602,\n",
       "         'woke': 123,\n",
       "         'vegetable': 24,\n",
       "         'patronizing': 10,\n",
       "         'words': 568,\n",
       "         'shitfaced': 7,\n",
       "         'affection': 25,\n",
       "         'blind': 205,\n",
       "         'hatred': 23,\n",
       "         'whyd': 194,\n",
       "         'itd': 158,\n",
       "         'mainline': 2,\n",
       "         'tequila': 20,\n",
       "         'laid': 171,\n",
       "         'above': 156,\n",
       "         'wake': 360,\n",
       "         'jail': 336,\n",
       "         'werent': 675,\n",
       "         'father': 2055,\n",
       "         ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])\n",
    "\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq = 5\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 18243\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words are: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/WORDMAP_corpus.json', 'w') as j:\n",
    "    json.dump(word_map, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(words, word_map):\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reply(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
    "    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append([qus, ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rev_word_map = {v: k for k, v in word_map.items()}\n",
    "# ' '.join([rev_word_map[v] for v in pairs_encoded[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.pairs = json.load(open('data/pairs_encoded.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "            \n",
    "        return question, reply\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
    "                                           batch_size = 100, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, reply = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = question!=0\n",
    "    question_mask = question_mask.to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
    "     \n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
    "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, encoded_words):\n",
    "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_words, src_mask):\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
    "        tgt_embeddings = self.embed(target_words)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 25\n",
    "\n",
    "with open('data/WORDMAP_corpus.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    \n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        \n",
    "        samples = question.shape[0]\n",
    "\n",
    "        # Move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "\n",
    "        # Create mask and add dimensions\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "\n",
    "        # Get the transformer outputs\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        \n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['<start>']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        \n",
    "    # Construct Sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/2217]\tLoss: 8.651\n",
      "Epoch [0][100/2217]\tLoss: 7.685\n",
      "Epoch [0][200/2217]\tLoss: 7.075\n",
      "Epoch [0][300/2217]\tLoss: 6.566\n",
      "Epoch [0][400/2217]\tLoss: 6.215\n",
      "Epoch [0][500/2217]\tLoss: 5.972\n",
      "Epoch [0][600/2217]\tLoss: 5.791\n",
      "Epoch [0][700/2217]\tLoss: 5.650\n",
      "Epoch [0][800/2217]\tLoss: 5.534\n",
      "Epoch [0][900/2217]\tLoss: 5.440\n",
      "Epoch [0][1000/2217]\tLoss: 5.361\n",
      "Epoch [0][1100/2217]\tLoss: 5.293\n",
      "Epoch [0][1200/2217]\tLoss: 5.234\n",
      "Epoch [0][1300/2217]\tLoss: 5.183\n",
      "Epoch [0][1400/2217]\tLoss: 5.139\n",
      "Epoch [0][1500/2217]\tLoss: 5.099\n",
      "Epoch [0][1600/2217]\tLoss: 5.063\n",
      "Epoch [0][1700/2217]\tLoss: 5.030\n",
      "Epoch [0][1800/2217]\tLoss: 5.000\n",
      "Epoch [0][1900/2217]\tLoss: 4.971\n",
      "Epoch [0][2000/2217]\tLoss: 4.946\n",
      "Epoch [0][2100/2217]\tLoss: 4.923\n",
      "Epoch [0][2200/2217]\tLoss: 4.902\n",
      "Epoch [1][0/2217]\tLoss: 4.459\n",
      "Epoch [1][100/2217]\tLoss: 4.410\n",
      "Epoch [1][200/2217]\tLoss: 4.405\n",
      "Epoch [1][300/2217]\tLoss: 4.406\n",
      "Epoch [1][400/2217]\tLoss: 4.407\n",
      "Epoch [1][500/2217]\tLoss: 4.407\n",
      "Epoch [1][600/2217]\tLoss: 4.406\n",
      "Epoch [1][700/2217]\tLoss: 4.405\n",
      "Epoch [1][800/2217]\tLoss: 4.404\n",
      "Epoch [1][900/2217]\tLoss: 4.402\n",
      "Epoch [1][1000/2217]\tLoss: 4.400\n",
      "Epoch [1][1100/2217]\tLoss: 4.400\n",
      "Epoch [1][1200/2217]\tLoss: 4.399\n",
      "Epoch [1][1300/2217]\tLoss: 4.398\n",
      "Epoch [1][1400/2217]\tLoss: 4.398\n",
      "Epoch [1][1500/2217]\tLoss: 4.397\n",
      "Epoch [1][1600/2217]\tLoss: 4.396\n",
      "Epoch [1][1700/2217]\tLoss: 4.394\n",
      "Epoch [1][1800/2217]\tLoss: 4.393\n",
      "Epoch [1][1900/2217]\tLoss: 4.393\n",
      "Epoch [1][2000/2217]\tLoss: 4.393\n",
      "Epoch [1][2100/2217]\tLoss: 4.392\n",
      "Epoch [1][2200/2217]\tLoss: 4.391\n",
      "Epoch [2][0/2217]\tLoss: 4.331\n",
      "Epoch [2][100/2217]\tLoss: 4.326\n",
      "Epoch [2][200/2217]\tLoss: 4.323\n",
      "Epoch [2][300/2217]\tLoss: 4.323\n",
      "Epoch [2][400/2217]\tLoss: 4.318\n",
      "Epoch [2][500/2217]\tLoss: 4.316\n",
      "Epoch [2][600/2217]\tLoss: 4.315\n",
      "Epoch [2][700/2217]\tLoss: 4.315\n",
      "Epoch [2][800/2217]\tLoss: 4.314\n",
      "Epoch [2][900/2217]\tLoss: 4.314\n",
      "Epoch [2][1000/2217]\tLoss: 4.313\n",
      "Epoch [2][1100/2217]\tLoss: 4.312\n",
      "Epoch [2][1200/2217]\tLoss: 4.310\n",
      "Epoch [2][1300/2217]\tLoss: 4.309\n",
      "Epoch [2][1400/2217]\tLoss: 4.308\n",
      "Epoch [2][1500/2217]\tLoss: 4.306\n",
      "Epoch [2][1600/2217]\tLoss: 4.304\n",
      "Epoch [2][1700/2217]\tLoss: 4.302\n",
      "Epoch [2][1800/2217]\tLoss: 4.301\n",
      "Epoch [2][1900/2217]\tLoss: 4.300\n",
      "Epoch [2][2000/2217]\tLoss: 4.300\n",
      "Epoch [2][2100/2217]\tLoss: 4.299\n",
      "Epoch [2][2200/2217]\tLoss: 4.298\n",
      "Epoch [3][0/2217]\tLoss: 4.083\n",
      "Epoch [3][100/2217]\tLoss: 4.184\n",
      "Epoch [3][200/2217]\tLoss: 4.183\n",
      "Epoch [3][300/2217]\tLoss: 4.188\n",
      "Epoch [3][400/2217]\tLoss: 4.199\n",
      "Epoch [3][500/2217]\tLoss: 4.200\n",
      "Epoch [3][600/2217]\tLoss: 4.202\n",
      "Epoch [3][700/2217]\tLoss: 4.202\n",
      "Epoch [3][800/2217]\tLoss: 4.204\n",
      "Epoch [3][900/2217]\tLoss: 4.205\n",
      "Epoch [3][1000/2217]\tLoss: 4.207\n",
      "Epoch [3][1100/2217]\tLoss: 4.209\n",
      "Epoch [3][1200/2217]\tLoss: 4.209\n",
      "Epoch [3][1300/2217]\tLoss: 4.210\n",
      "Epoch [3][1400/2217]\tLoss: 4.211\n",
      "Epoch [3][1500/2217]\tLoss: 4.212\n",
      "Epoch [3][1600/2217]\tLoss: 4.212\n",
      "Epoch [3][1700/2217]\tLoss: 4.212\n",
      "Epoch [3][1800/2217]\tLoss: 4.212\n",
      "Epoch [3][1900/2217]\tLoss: 4.211\n",
      "Epoch [3][2000/2217]\tLoss: 4.211\n",
      "Epoch [3][2100/2217]\tLoss: 4.210\n",
      "Epoch [3][2200/2217]\tLoss: 4.210\n",
      "Epoch [4][0/2217]\tLoss: 3.984\n",
      "Epoch [4][100/2217]\tLoss: 4.121\n",
      "Epoch [4][200/2217]\tLoss: 4.127\n",
      "Epoch [4][300/2217]\tLoss: 4.127\n",
      "Epoch [4][400/2217]\tLoss: 4.127\n",
      "Epoch [4][500/2217]\tLoss: 4.127\n",
      "Epoch [4][600/2217]\tLoss: 4.129\n",
      "Epoch [4][700/2217]\tLoss: 4.131\n",
      "Epoch [4][800/2217]\tLoss: 4.135\n",
      "Epoch [4][900/2217]\tLoss: 4.137\n",
      "Epoch [4][1000/2217]\tLoss: 4.138\n",
      "Epoch [4][1100/2217]\tLoss: 4.138\n",
      "Epoch [4][1200/2217]\tLoss: 4.138\n",
      "Epoch [4][1300/2217]\tLoss: 4.138\n",
      "Epoch [4][1400/2217]\tLoss: 4.140\n",
      "Epoch [4][1500/2217]\tLoss: 4.141\n",
      "Epoch [4][1600/2217]\tLoss: 4.141\n",
      "Epoch [4][1700/2217]\tLoss: 4.142\n",
      "Epoch [4][1800/2217]\tLoss: 4.142\n",
      "Epoch [4][1900/2217]\tLoss: 4.142\n",
      "Epoch [4][2000/2217]\tLoss: 4.142\n",
      "Epoch [4][2100/2217]\tLoss: 4.142\n",
      "Epoch [4][2200/2217]\tLoss: 4.143\n",
      "Epoch [5][0/2217]\tLoss: 3.996\n",
      "Epoch [5][100/2217]\tLoss: 4.072\n",
      "Epoch [5][200/2217]\tLoss: 4.067\n",
      "Epoch [5][300/2217]\tLoss: 4.071\n",
      "Epoch [5][400/2217]\tLoss: 4.070\n",
      "Epoch [5][500/2217]\tLoss: 4.074\n",
      "Epoch [5][600/2217]\tLoss: 4.073\n",
      "Epoch [5][700/2217]\tLoss: 4.077\n",
      "Epoch [5][800/2217]\tLoss: 4.078\n",
      "Epoch [5][900/2217]\tLoss: 4.079\n",
      "Epoch [5][1000/2217]\tLoss: 4.079\n",
      "Epoch [5][1100/2217]\tLoss: 4.081\n",
      "Epoch [5][1200/2217]\tLoss: 4.081\n",
      "Epoch [5][1300/2217]\tLoss: 4.082\n",
      "Epoch [5][1400/2217]\tLoss: 4.084\n",
      "Epoch [5][1500/2217]\tLoss: 4.085\n",
      "Epoch [5][1600/2217]\tLoss: 4.085\n",
      "Epoch [5][1700/2217]\tLoss: 4.085\n",
      "Epoch [5][1800/2217]\tLoss: 4.086\n",
      "Epoch [5][1900/2217]\tLoss: 4.086\n",
      "Epoch [5][2000/2217]\tLoss: 4.086\n",
      "Epoch [5][2100/2217]\tLoss: 4.086\n",
      "Epoch [5][2200/2217]\tLoss: 4.087\n",
      "Epoch [6][0/2217]\tLoss: 3.982\n",
      "Epoch [6][100/2217]\tLoss: 4.000\n",
      "Epoch [6][200/2217]\tLoss: 4.007\n",
      "Epoch [6][300/2217]\tLoss: 4.006\n",
      "Epoch [6][400/2217]\tLoss: 4.010\n",
      "Epoch [6][500/2217]\tLoss: 4.011\n",
      "Epoch [6][600/2217]\tLoss: 4.012\n",
      "Epoch [6][700/2217]\tLoss: 4.014\n",
      "Epoch [6][800/2217]\tLoss: 4.019\n",
      "Epoch [6][900/2217]\tLoss: 4.021\n",
      "Epoch [6][1000/2217]\tLoss: 4.022\n",
      "Epoch [6][1100/2217]\tLoss: 4.024\n",
      "Epoch [6][1200/2217]\tLoss: 4.025\n",
      "Epoch [6][1300/2217]\tLoss: 4.027\n",
      "Epoch [6][1400/2217]\tLoss: 4.029\n",
      "Epoch [6][1500/2217]\tLoss: 4.031\n",
      "Epoch [6][1600/2217]\tLoss: 4.032\n",
      "Epoch [6][1700/2217]\tLoss: 4.033\n",
      "Epoch [6][1800/2217]\tLoss: 4.034\n",
      "Epoch [6][1900/2217]\tLoss: 4.035\n",
      "Epoch [6][2000/2217]\tLoss: 4.036\n",
      "Epoch [6][2100/2217]\tLoss: 4.037\n",
      "Epoch [6][2200/2217]\tLoss: 4.039\n",
      "Epoch [7][0/2217]\tLoss: 3.957\n",
      "Epoch [7][100/2217]\tLoss: 3.960\n",
      "Epoch [7][200/2217]\tLoss: 3.962\n",
      "Epoch [7][300/2217]\tLoss: 3.969\n",
      "Epoch [7][400/2217]\tLoss: 3.967\n",
      "Epoch [7][500/2217]\tLoss: 3.965\n",
      "Epoch [7][600/2217]\tLoss: 3.968\n",
      "Epoch [7][700/2217]\tLoss: 3.970\n",
      "Epoch [7][800/2217]\tLoss: 3.975\n",
      "Epoch [7][900/2217]\tLoss: 3.976\n",
      "Epoch [7][1000/2217]\tLoss: 3.977\n",
      "Epoch [7][1100/2217]\tLoss: 3.979\n",
      "Epoch [7][1200/2217]\tLoss: 3.981\n",
      "Epoch [7][1300/2217]\tLoss: 3.982\n",
      "Epoch [7][1400/2217]\tLoss: 3.983\n",
      "Epoch [7][1500/2217]\tLoss: 3.985\n",
      "Epoch [7][1600/2217]\tLoss: 3.986\n",
      "Epoch [7][1700/2217]\tLoss: 3.987\n",
      "Epoch [7][1800/2217]\tLoss: 3.989\n",
      "Epoch [7][1900/2217]\tLoss: 3.990\n",
      "Epoch [7][2000/2217]\tLoss: 3.991\n",
      "Epoch [7][2100/2217]\tLoss: 3.992\n",
      "Epoch [7][2200/2217]\tLoss: 3.994\n",
      "Epoch [8][0/2217]\tLoss: 3.831\n",
      "Epoch [8][100/2217]\tLoss: 3.896\n",
      "Epoch [8][200/2217]\tLoss: 3.910\n",
      "Epoch [8][300/2217]\tLoss: 3.919\n",
      "Epoch [8][400/2217]\tLoss: 3.922\n",
      "Epoch [8][500/2217]\tLoss: 3.929\n",
      "Epoch [8][600/2217]\tLoss: 3.929\n",
      "Epoch [8][700/2217]\tLoss: 3.931\n",
      "Epoch [8][800/2217]\tLoss: 3.934\n",
      "Epoch [8][900/2217]\tLoss: 3.937\n",
      "Epoch [8][1000/2217]\tLoss: 3.939\n",
      "Epoch [8][1100/2217]\tLoss: 3.941\n",
      "Epoch [8][1200/2217]\tLoss: 3.942\n",
      "Epoch [8][1300/2217]\tLoss: 3.943\n",
      "Epoch [8][1400/2217]\tLoss: 3.945\n",
      "Epoch [8][1500/2217]\tLoss: 3.946\n",
      "Epoch [8][1600/2217]\tLoss: 3.947\n",
      "Epoch [8][1700/2217]\tLoss: 3.949\n",
      "Epoch [8][1800/2217]\tLoss: 3.950\n",
      "Epoch [8][1900/2217]\tLoss: 3.951\n",
      "Epoch [8][2000/2217]\tLoss: 3.953\n",
      "Epoch [8][2100/2217]\tLoss: 3.954\n",
      "Epoch [8][2200/2217]\tLoss: 3.955\n",
      "Epoch [9][0/2217]\tLoss: 3.975\n",
      "Epoch [9][100/2217]\tLoss: 3.871\n",
      "Epoch [9][200/2217]\tLoss: 3.870\n",
      "Epoch [9][300/2217]\tLoss: 3.878\n",
      "Epoch [9][400/2217]\tLoss: 3.883\n",
      "Epoch [9][500/2217]\tLoss: 3.884\n",
      "Epoch [9][600/2217]\tLoss: 3.887\n",
      "Epoch [9][700/2217]\tLoss: 3.890\n",
      "Epoch [9][800/2217]\tLoss: 3.891\n",
      "Epoch [9][900/2217]\tLoss: 3.894\n",
      "Epoch [9][1000/2217]\tLoss: 3.897\n",
      "Epoch [9][1100/2217]\tLoss: 3.900\n",
      "Epoch [9][1200/2217]\tLoss: 3.902\n",
      "Epoch [9][1300/2217]\tLoss: 3.903\n",
      "Epoch [9][1400/2217]\tLoss: 3.904\n",
      "Epoch [9][1500/2217]\tLoss: 3.907\n",
      "Epoch [9][1600/2217]\tLoss: 3.908\n",
      "Epoch [9][1700/2217]\tLoss: 3.910\n",
      "Epoch [9][1800/2217]\tLoss: 3.911\n",
      "Epoch [9][1900/2217]\tLoss: 3.913\n",
      "Epoch [9][2000/2217]\tLoss: 3.915\n",
      "Epoch [9][2100/2217]\tLoss: 3.917\n",
      "Epoch [9][2200/2217]\tLoss: 3.918\n",
      "Epoch [10][0/2217]\tLoss: 3.677\n",
      "Epoch [10][100/2217]\tLoss: 3.830\n",
      "Epoch [10][200/2217]\tLoss: 3.830\n",
      "Epoch [10][300/2217]\tLoss: 3.840\n",
      "Epoch [10][400/2217]\tLoss: 3.845\n",
      "Epoch [10][500/2217]\tLoss: 3.845\n",
      "Epoch [10][600/2217]\tLoss: 3.851\n",
      "Epoch [10][700/2217]\tLoss: 3.854\n",
      "Epoch [10][800/2217]\tLoss: 3.857\n",
      "Epoch [10][900/2217]\tLoss: 3.861\n",
      "Epoch [10][1000/2217]\tLoss: 3.863\n",
      "Epoch [10][1100/2217]\tLoss: 3.865\n",
      "Epoch [10][1200/2217]\tLoss: 3.867\n",
      "Epoch [10][1300/2217]\tLoss: 3.870\n",
      "Epoch [10][1400/2217]\tLoss: 3.871\n",
      "Epoch [10][1500/2217]\tLoss: 3.873\n",
      "Epoch [10][1600/2217]\tLoss: 3.875\n",
      "Epoch [10][1700/2217]\tLoss: 3.876\n",
      "Epoch [10][1800/2217]\tLoss: 3.877\n",
      "Epoch [10][1900/2217]\tLoss: 3.880\n",
      "Epoch [10][2000/2217]\tLoss: 3.880\n",
      "Epoch [10][2100/2217]\tLoss: 3.882\n",
      "Epoch [10][2200/2217]\tLoss: 3.883\n",
      "Epoch [11][0/2217]\tLoss: 3.794\n",
      "Epoch [11][100/2217]\tLoss: 3.803\n",
      "Epoch [11][200/2217]\tLoss: 3.795\n",
      "Epoch [11][300/2217]\tLoss: 3.795\n",
      "Epoch [11][400/2217]\tLoss: 3.802\n",
      "Epoch [11][500/2217]\tLoss: 3.807\n",
      "Epoch [11][600/2217]\tLoss: 3.811\n",
      "Epoch [11][700/2217]\tLoss: 3.813\n",
      "Epoch [11][800/2217]\tLoss: 3.817\n",
      "Epoch [11][900/2217]\tLoss: 3.820\n",
      "Epoch [11][1000/2217]\tLoss: 3.824\n",
      "Epoch [11][1100/2217]\tLoss: 3.827\n",
      "Epoch [11][1200/2217]\tLoss: 3.828\n",
      "Epoch [11][1300/2217]\tLoss: 3.831\n",
      "Epoch [11][1400/2217]\tLoss: 3.834\n",
      "Epoch [11][1500/2217]\tLoss: 3.836\n",
      "Epoch [11][1600/2217]\tLoss: 3.838\n",
      "Epoch [11][1700/2217]\tLoss: 3.840\n",
      "Epoch [11][1800/2217]\tLoss: 3.843\n",
      "Epoch [11][1900/2217]\tLoss: 3.845\n",
      "Epoch [11][2000/2217]\tLoss: 3.847\n",
      "Epoch [11][2100/2217]\tLoss: 3.849\n",
      "Epoch [11][2200/2217]\tLoss: 3.850\n",
      "Epoch [12][0/2217]\tLoss: 3.810\n",
      "Epoch [12][100/2217]\tLoss: 3.753\n",
      "Epoch [12][200/2217]\tLoss: 3.755\n",
      "Epoch [12][300/2217]\tLoss: 3.766\n",
      "Epoch [12][400/2217]\tLoss: 3.777\n",
      "Epoch [12][500/2217]\tLoss: 3.779\n",
      "Epoch [12][600/2217]\tLoss: 3.782\n",
      "Epoch [12][700/2217]\tLoss: 3.786\n",
      "Epoch [12][800/2217]\tLoss: 3.787\n",
      "Epoch [12][900/2217]\tLoss: 3.790\n",
      "Epoch [12][1000/2217]\tLoss: 3.793\n",
      "Epoch [12][1100/2217]\tLoss: 3.797\n",
      "Epoch [12][1200/2217]\tLoss: 3.798\n",
      "Epoch [12][1300/2217]\tLoss: 3.800\n",
      "Epoch [12][1400/2217]\tLoss: 3.804\n",
      "Epoch [12][1500/2217]\tLoss: 3.806\n",
      "Epoch [12][1600/2217]\tLoss: 3.808\n",
      "Epoch [12][1700/2217]\tLoss: 3.810\n",
      "Epoch [12][1800/2217]\tLoss: 3.811\n",
      "Epoch [12][1900/2217]\tLoss: 3.813\n",
      "Epoch [12][2000/2217]\tLoss: 3.815\n",
      "Epoch [12][2100/2217]\tLoss: 3.817\n",
      "Epoch [12][2200/2217]\tLoss: 3.819\n",
      "Epoch [13][0/2217]\tLoss: 3.719\n",
      "Epoch [13][100/2217]\tLoss: 3.725\n",
      "Epoch [13][200/2217]\tLoss: 3.731\n",
      "Epoch [13][300/2217]\tLoss: 3.736\n",
      "Epoch [13][400/2217]\tLoss: 3.743\n",
      "Epoch [13][500/2217]\tLoss: 3.747\n",
      "Epoch [13][600/2217]\tLoss: 3.750\n",
      "Epoch [13][700/2217]\tLoss: 3.754\n",
      "Epoch [13][800/2217]\tLoss: 3.757\n",
      "Epoch [13][900/2217]\tLoss: 3.758\n",
      "Epoch [13][1000/2217]\tLoss: 3.762\n",
      "Epoch [13][1100/2217]\tLoss: 3.766\n",
      "Epoch [13][1200/2217]\tLoss: 3.770\n",
      "Epoch [13][1300/2217]\tLoss: 3.772\n",
      "Epoch [13][1400/2217]\tLoss: 3.775\n",
      "Epoch [13][1500/2217]\tLoss: 3.778\n",
      "Epoch [13][1600/2217]\tLoss: 3.779\n",
      "Epoch [13][1700/2217]\tLoss: 3.780\n",
      "Epoch [13][1800/2217]\tLoss: 3.782\n",
      "Epoch [13][1900/2217]\tLoss: 3.784\n",
      "Epoch [13][2000/2217]\tLoss: 3.786\n",
      "Epoch [13][2100/2217]\tLoss: 3.788\n",
      "Epoch [13][2200/2217]\tLoss: 3.789\n",
      "Epoch [14][0/2217]\tLoss: 3.670\n",
      "Epoch [14][100/2217]\tLoss: 3.714\n",
      "Epoch [14][200/2217]\tLoss: 3.708\n",
      "Epoch [14][300/2217]\tLoss: 3.717\n",
      "Epoch [14][400/2217]\tLoss: 3.718\n",
      "Epoch [14][500/2217]\tLoss: 3.722\n",
      "Epoch [14][600/2217]\tLoss: 3.725\n",
      "Epoch [14][700/2217]\tLoss: 3.729\n",
      "Epoch [14][800/2217]\tLoss: 3.730\n",
      "Epoch [14][900/2217]\tLoss: 3.732\n",
      "Epoch [14][1000/2217]\tLoss: 3.734\n",
      "Epoch [14][1100/2217]\tLoss: 3.738\n",
      "Epoch [14][1200/2217]\tLoss: 3.741\n",
      "Epoch [14][1300/2217]\tLoss: 3.744\n",
      "Epoch [14][1400/2217]\tLoss: 3.746\n",
      "Epoch [14][1500/2217]\tLoss: 3.748\n",
      "Epoch [14][1600/2217]\tLoss: 3.749\n",
      "Epoch [14][1700/2217]\tLoss: 3.751\n",
      "Epoch [14][1800/2217]\tLoss: 3.753\n",
      "Epoch [14][1900/2217]\tLoss: 3.755\n",
      "Epoch [14][2000/2217]\tLoss: 3.756\n",
      "Epoch [14][2100/2217]\tLoss: 3.759\n",
      "Epoch [14][2200/2217]\tLoss: 3.761\n",
      "Epoch [15][0/2217]\tLoss: 3.902\n",
      "Epoch [15][100/2217]\tLoss: 3.662\n",
      "Epoch [15][200/2217]\tLoss: 3.670\n",
      "Epoch [15][300/2217]\tLoss: 3.674\n",
      "Epoch [15][400/2217]\tLoss: 3.681\n",
      "Epoch [15][500/2217]\tLoss: 3.684\n",
      "Epoch [15][600/2217]\tLoss: 3.688\n",
      "Epoch [15][700/2217]\tLoss: 3.693\n",
      "Epoch [15][800/2217]\tLoss: 3.699\n",
      "Epoch [15][900/2217]\tLoss: 3.702\n",
      "Epoch [15][1000/2217]\tLoss: 3.705\n",
      "Epoch [15][1100/2217]\tLoss: 3.708\n",
      "Epoch [15][1200/2217]\tLoss: 3.711\n",
      "Epoch [15][1300/2217]\tLoss: 3.714\n",
      "Epoch [15][1400/2217]\tLoss: 3.716\n",
      "Epoch [15][1500/2217]\tLoss: 3.718\n",
      "Epoch [15][1600/2217]\tLoss: 3.721\n",
      "Epoch [15][1700/2217]\tLoss: 3.724\n",
      "Epoch [15][1800/2217]\tLoss: 3.726\n",
      "Epoch [15][1900/2217]\tLoss: 3.728\n",
      "Epoch [15][2000/2217]\tLoss: 3.730\n",
      "Epoch [15][2100/2217]\tLoss: 3.732\n",
      "Epoch [15][2200/2217]\tLoss: 3.734\n",
      "Epoch [16][0/2217]\tLoss: 3.679\n",
      "Epoch [16][100/2217]\tLoss: 3.658\n",
      "Epoch [16][200/2217]\tLoss: 3.660\n",
      "Epoch [16][300/2217]\tLoss: 3.664\n",
      "Epoch [16][400/2217]\tLoss: 3.664\n",
      "Epoch [16][500/2217]\tLoss: 3.666\n",
      "Epoch [16][600/2217]\tLoss: 3.669\n",
      "Epoch [16][700/2217]\tLoss: 3.671\n",
      "Epoch [16][800/2217]\tLoss: 3.675\n",
      "Epoch [16][900/2217]\tLoss: 3.677\n",
      "Epoch [16][1000/2217]\tLoss: 3.681\n",
      "Epoch [16][1100/2217]\tLoss: 3.685\n",
      "Epoch [16][1200/2217]\tLoss: 3.688\n",
      "Epoch [16][1300/2217]\tLoss: 3.690\n",
      "Epoch [16][1400/2217]\tLoss: 3.692\n",
      "Epoch [16][1500/2217]\tLoss: 3.694\n",
      "Epoch [16][1600/2217]\tLoss: 3.697\n",
      "Epoch [16][1700/2217]\tLoss: 3.699\n",
      "Epoch [16][1800/2217]\tLoss: 3.700\n",
      "Epoch [16][1900/2217]\tLoss: 3.702\n",
      "Epoch [16][2000/2217]\tLoss: 3.705\n",
      "Epoch [16][2100/2217]\tLoss: 3.706\n",
      "Epoch [16][2200/2217]\tLoss: 3.708\n",
      "Epoch [17][0/2217]\tLoss: 3.615\n",
      "Epoch [17][100/2217]\tLoss: 3.611\n",
      "Epoch [17][200/2217]\tLoss: 3.621\n",
      "Epoch [17][300/2217]\tLoss: 3.632\n",
      "Epoch [17][400/2217]\tLoss: 3.638\n",
      "Epoch [17][500/2217]\tLoss: 3.641\n",
      "Epoch [17][600/2217]\tLoss: 3.644\n",
      "Epoch [17][700/2217]\tLoss: 3.646\n",
      "Epoch [17][800/2217]\tLoss: 3.648\n",
      "Epoch [17][900/2217]\tLoss: 3.651\n",
      "Epoch [17][1000/2217]\tLoss: 3.656\n",
      "Epoch [17][1100/2217]\tLoss: 3.658\n",
      "Epoch [17][1200/2217]\tLoss: 3.660\n",
      "Epoch [17][1300/2217]\tLoss: 3.663\n",
      "Epoch [17][1400/2217]\tLoss: 3.665\n",
      "Epoch [17][1500/2217]\tLoss: 3.667\n",
      "Epoch [17][1600/2217]\tLoss: 3.670\n",
      "Epoch [17][1700/2217]\tLoss: 3.673\n",
      "Epoch [17][1800/2217]\tLoss: 3.675\n",
      "Epoch [17][1900/2217]\tLoss: 3.678\n",
      "Epoch [17][2000/2217]\tLoss: 3.680\n",
      "Epoch [17][2100/2217]\tLoss: 3.682\n",
      "Epoch [17][2200/2217]\tLoss: 3.684\n",
      "Epoch [18][0/2217]\tLoss: 3.532\n",
      "Epoch [18][100/2217]\tLoss: 3.593\n",
      "Epoch [18][200/2217]\tLoss: 3.592\n",
      "Epoch [18][300/2217]\tLoss: 3.600\n",
      "Epoch [18][400/2217]\tLoss: 3.606\n",
      "Epoch [18][500/2217]\tLoss: 3.610\n",
      "Epoch [18][600/2217]\tLoss: 3.614\n",
      "Epoch [18][700/2217]\tLoss: 3.619\n",
      "Epoch [18][800/2217]\tLoss: 3.625\n",
      "Epoch [18][900/2217]\tLoss: 3.627\n",
      "Epoch [18][1000/2217]\tLoss: 3.629\n",
      "Epoch [18][1100/2217]\tLoss: 3.633\n",
      "Epoch [18][1200/2217]\tLoss: 3.636\n",
      "Epoch [18][1300/2217]\tLoss: 3.640\n",
      "Epoch [18][1400/2217]\tLoss: 3.643\n",
      "Epoch [18][1500/2217]\tLoss: 3.645\n",
      "Epoch [18][1600/2217]\tLoss: 3.648\n",
      "Epoch [18][1700/2217]\tLoss: 3.650\n",
      "Epoch [18][1800/2217]\tLoss: 3.653\n",
      "Epoch [18][1900/2217]\tLoss: 3.655\n",
      "Epoch [18][2000/2217]\tLoss: 3.656\n",
      "Epoch [18][2100/2217]\tLoss: 3.659\n",
      "Epoch [18][2200/2217]\tLoss: 3.661\n",
      "Epoch [19][0/2217]\tLoss: 3.688\n",
      "Epoch [19][100/2217]\tLoss: 3.563\n",
      "Epoch [19][200/2217]\tLoss: 3.567\n",
      "Epoch [19][300/2217]\tLoss: 3.572\n",
      "Epoch [19][400/2217]\tLoss: 3.578\n",
      "Epoch [19][500/2217]\tLoss: 3.583\n",
      "Epoch [19][600/2217]\tLoss: 3.590\n",
      "Epoch [19][700/2217]\tLoss: 3.594\n",
      "Epoch [19][800/2217]\tLoss: 3.599\n",
      "Epoch [19][900/2217]\tLoss: 3.601\n",
      "Epoch [19][1000/2217]\tLoss: 3.606\n",
      "Epoch [19][1100/2217]\tLoss: 3.609\n",
      "Epoch [19][1200/2217]\tLoss: 3.613\n",
      "Epoch [19][1300/2217]\tLoss: 3.617\n",
      "Epoch [19][1400/2217]\tLoss: 3.620\n",
      "Epoch [19][1500/2217]\tLoss: 3.622\n",
      "Epoch [19][1600/2217]\tLoss: 3.625\n",
      "Epoch [19][1700/2217]\tLoss: 3.628\n",
      "Epoch [19][1800/2217]\tLoss: 3.630\n",
      "Epoch [19][1900/2217]\tLoss: 3.632\n",
      "Epoch [19][2000/2217]\tLoss: 3.633\n",
      "Epoch [19][2100/2217]\tLoss: 3.635\n",
      "Epoch [19][2200/2217]\tLoss: 3.637\n",
      "Epoch [20][0/2217]\tLoss: 3.586\n",
      "Epoch [20][100/2217]\tLoss: 3.563\n",
      "Epoch [20][200/2217]\tLoss: 3.556\n",
      "Epoch [20][300/2217]\tLoss: 3.559\n",
      "Epoch [20][400/2217]\tLoss: 3.564\n",
      "Epoch [20][500/2217]\tLoss: 3.566\n",
      "Epoch [20][600/2217]\tLoss: 3.571\n",
      "Epoch [20][700/2217]\tLoss: 3.571\n",
      "Epoch [20][800/2217]\tLoss: 3.575\n",
      "Epoch [20][900/2217]\tLoss: 3.578\n",
      "Epoch [20][1000/2217]\tLoss: 3.582\n",
      "Epoch [20][1100/2217]\tLoss: 3.586\n",
      "Epoch [20][1200/2217]\tLoss: 3.589\n",
      "Epoch [20][1300/2217]\tLoss: 3.592\n",
      "Epoch [20][1400/2217]\tLoss: 3.595\n",
      "Epoch [20][1500/2217]\tLoss: 3.598\n",
      "Epoch [20][1600/2217]\tLoss: 3.601\n",
      "Epoch [20][1700/2217]\tLoss: 3.604\n",
      "Epoch [20][1800/2217]\tLoss: 3.607\n",
      "Epoch [20][1900/2217]\tLoss: 3.609\n",
      "Epoch [20][2000/2217]\tLoss: 3.611\n",
      "Epoch [20][2100/2217]\tLoss: 3.613\n",
      "Epoch [20][2200/2217]\tLoss: 3.616\n",
      "Epoch [21][0/2217]\tLoss: 3.392\n",
      "Epoch [21][100/2217]\tLoss: 3.525\n",
      "Epoch [21][200/2217]\tLoss: 3.533\n",
      "Epoch [21][300/2217]\tLoss: 3.536\n",
      "Epoch [21][400/2217]\tLoss: 3.543\n",
      "Epoch [21][500/2217]\tLoss: 3.548\n",
      "Epoch [21][600/2217]\tLoss: 3.551\n",
      "Epoch [21][700/2217]\tLoss: 3.554\n",
      "Epoch [21][800/2217]\tLoss: 3.557\n",
      "Epoch [21][900/2217]\tLoss: 3.560\n",
      "Epoch [21][1000/2217]\tLoss: 3.562\n",
      "Epoch [21][1100/2217]\tLoss: 3.564\n",
      "Epoch [21][1200/2217]\tLoss: 3.567\n",
      "Epoch [21][1300/2217]\tLoss: 3.570\n",
      "Epoch [21][1400/2217]\tLoss: 3.572\n",
      "Epoch [21][1500/2217]\tLoss: 3.575\n",
      "Epoch [21][1600/2217]\tLoss: 3.579\n",
      "Epoch [21][1700/2217]\tLoss: 3.581\n",
      "Epoch [21][1800/2217]\tLoss: 3.585\n",
      "Epoch [21][1900/2217]\tLoss: 3.588\n",
      "Epoch [21][2000/2217]\tLoss: 3.589\n",
      "Epoch [21][2100/2217]\tLoss: 3.592\n",
      "Epoch [21][2200/2217]\tLoss: 3.594\n",
      "Epoch [22][0/2217]\tLoss: 3.516\n",
      "Epoch [22][100/2217]\tLoss: 3.508\n",
      "Epoch [22][200/2217]\tLoss: 3.509\n",
      "Epoch [22][300/2217]\tLoss: 3.512\n",
      "Epoch [22][400/2217]\tLoss: 3.518\n",
      "Epoch [22][500/2217]\tLoss: 3.524\n",
      "Epoch [22][600/2217]\tLoss: 3.526\n",
      "Epoch [22][700/2217]\tLoss: 3.531\n",
      "Epoch [22][800/2217]\tLoss: 3.533\n",
      "Epoch [22][900/2217]\tLoss: 3.538\n",
      "Epoch [22][1000/2217]\tLoss: 3.541\n",
      "Epoch [22][1100/2217]\tLoss: 3.544\n",
      "Epoch [22][1200/2217]\tLoss: 3.546\n",
      "Epoch [22][1300/2217]\tLoss: 3.549\n",
      "Epoch [22][1400/2217]\tLoss: 3.551\n",
      "Epoch [22][1500/2217]\tLoss: 3.555\n",
      "Epoch [22][1600/2217]\tLoss: 3.557\n",
      "Epoch [22][1700/2217]\tLoss: 3.560\n",
      "Epoch [22][1800/2217]\tLoss: 3.562\n",
      "Epoch [22][1900/2217]\tLoss: 3.565\n",
      "Epoch [22][2000/2217]\tLoss: 3.567\n",
      "Epoch [22][2100/2217]\tLoss: 3.569\n",
      "Epoch [22][2200/2217]\tLoss: 3.572\n",
      "Epoch [23][0/2217]\tLoss: 3.282\n",
      "Epoch [23][100/2217]\tLoss: 3.474\n",
      "Epoch [23][200/2217]\tLoss: 3.483\n",
      "Epoch [23][300/2217]\tLoss: 3.490\n",
      "Epoch [23][400/2217]\tLoss: 3.496\n",
      "Epoch [23][500/2217]\tLoss: 3.499\n",
      "Epoch [23][600/2217]\tLoss: 3.504\n",
      "Epoch [23][700/2217]\tLoss: 3.509\n",
      "Epoch [23][800/2217]\tLoss: 3.512\n",
      "Epoch [23][900/2217]\tLoss: 3.516\n",
      "Epoch [23][1000/2217]\tLoss: 3.520\n",
      "Epoch [23][1100/2217]\tLoss: 3.522\n",
      "Epoch [23][1200/2217]\tLoss: 3.526\n",
      "Epoch [23][1300/2217]\tLoss: 3.529\n",
      "Epoch [23][1400/2217]\tLoss: 3.532\n",
      "Epoch [23][1500/2217]\tLoss: 3.535\n",
      "Epoch [23][1600/2217]\tLoss: 3.538\n",
      "Epoch [23][1700/2217]\tLoss: 3.540\n",
      "Epoch [23][1800/2217]\tLoss: 3.543\n",
      "Epoch [23][1900/2217]\tLoss: 3.545\n",
      "Epoch [23][2000/2217]\tLoss: 3.548\n",
      "Epoch [23][2100/2217]\tLoss: 3.551\n",
      "Epoch [23][2200/2217]\tLoss: 3.552\n",
      "Epoch [24][0/2217]\tLoss: 3.415\n",
      "Epoch [24][100/2217]\tLoss: 3.462\n",
      "Epoch [24][200/2217]\tLoss: 3.469\n",
      "Epoch [24][300/2217]\tLoss: 3.473\n",
      "Epoch [24][400/2217]\tLoss: 3.478\n",
      "Epoch [24][500/2217]\tLoss: 3.481\n",
      "Epoch [24][600/2217]\tLoss: 3.484\n",
      "Epoch [24][700/2217]\tLoss: 3.489\n",
      "Epoch [24][800/2217]\tLoss: 3.494\n",
      "Epoch [24][900/2217]\tLoss: 3.497\n",
      "Epoch [24][1000/2217]\tLoss: 3.501\n",
      "Epoch [24][1100/2217]\tLoss: 3.505\n",
      "Epoch [24][1200/2217]\tLoss: 3.508\n",
      "Epoch [24][1300/2217]\tLoss: 3.511\n",
      "Epoch [24][1400/2217]\tLoss: 3.513\n",
      "Epoch [24][1500/2217]\tLoss: 3.517\n",
      "Epoch [24][1600/2217]\tLoss: 3.520\n",
      "Epoch [24][1700/2217]\tLoss: 3.522\n",
      "Epoch [24][1800/2217]\tLoss: 3.524\n",
      "Epoch [24][1900/2217]\tLoss: 3.526\n",
      "Epoch [24][2000/2217]\tLoss: 3.529\n",
      "Epoch [24][2100/2217]\tLoss: 3.532\n",
      "Epoch [24][2200/2217]\tLoss: 3.534\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    \n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint_24.pth.tar')\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i dont know\n",
      "i dont know\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    max_len = input(\"Maximum Reply Length: \")\n",
    "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "mr\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "while (True):\n",
    "    if question == 'quit':\n",
    "        break\n",
    "    question = input(\"Question: \")\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
