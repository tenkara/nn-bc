{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "TORCH_USE_CUDA_DSA=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_conv = 'data/cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "corpus_movie_lines = 'data/cornell movie-dialogs corpus/movie_lines.txt'\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    conv = c.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_lines, 'r') as l:\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dict[objects[0]] = objects[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They do not!\\n'"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_dict['L1045']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punc(string):\n",
    "#     punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "#     no_punctuation_string = \"\"\n",
    "#     for char in string:\n",
    "#         if char not in punctuations:\n",
    "#             no_punctuation_string = no_punctuation_string + char\n",
    "#     return no_punctuation_string.lower()\n",
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char  # space is also a character\n",
    "    return no_punct.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they do not\\n'"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ele = remove_punc(lines_dict['L1045'])\n",
    "ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221616"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "for con in conv:\n",
    "    \n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "\n",
    "        if i==len(ids)-1:\n",
    "            break\n",
    "        \n",
    "        first = remove_punc(lines_dict[ids[i]].strip())      \n",
    "        second = remove_punc(lines_dict[ids[i+1]].strip())\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        pairs.append(qa_pairs)\n",
    "\n",
    "        # qa_pairs.append(remove_punc(lines_dict[(ids[i])].strip()).split()[:max_len])\n",
    "        # qa_pairs.append(remove_punc(lines_dict[(ids[i+1])].strip()).split()[:max_len])\n",
    "        # pairs.append(qa_pairs)\n",
    "\n",
    "# pairs\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count = Counter()\n",
    "# for pair in pairs:\n",
    "#     for word in pair[0]:\n",
    "#         word_count[word] += 1\n",
    "#     for word in pair[1]:\n",
    "#         word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'can': 14103,\n",
       "         'we': 25912,\n",
       "         'make': 5821,\n",
       "         'this': 30502,\n",
       "         'quick': 310,\n",
       "         'roxanne': 1,\n",
       "         'korrine': 1,\n",
       "         'and': 52128,\n",
       "         'andrew': 49,\n",
       "         'barrett': 20,\n",
       "         'are': 21713,\n",
       "         'having': 1081,\n",
       "         'an': 8827,\n",
       "         'incredibly': 49,\n",
       "         'horrendous': 4,\n",
       "         'public': 306,\n",
       "         'break': 799,\n",
       "         'up': 14316,\n",
       "         'on': 23908,\n",
       "         'the': 120903,\n",
       "         'quad': 2,\n",
       "         'again': 2807,\n",
       "         'well': 16263,\n",
       "         'i': 137633,\n",
       "         'thought': 4202,\n",
       "         'wed': 541,\n",
       "         'start': 1459,\n",
       "         'with': 21394,\n",
       "         'pronunciation': 2,\n",
       "         'if': 16727,\n",
       "         'thats': 14742,\n",
       "         'okay': 5946,\n",
       "         'you': 169693,\n",
       "         'not': 26494,\n",
       "         'hacking': 18,\n",
       "         'gagging': 9,\n",
       "         'spitting': 15,\n",
       "         'part': 1260,\n",
       "         'please': 3258,\n",
       "         'then': 7532,\n",
       "         'how': 14001,\n",
       "         'bout': 393,\n",
       "         'try': 1998,\n",
       "         'out': 16100,\n",
       "         'some': 8153,\n",
       "         'french': 306,\n",
       "         'cuisine': 11,\n",
       "         'saturday': 184,\n",
       "         'night': 3489,\n",
       "         'youre': 18178,\n",
       "         'asking': 691,\n",
       "         'me': 41056,\n",
       "         'so': 16699,\n",
       "         'cute': 259,\n",
       "         'whats': 6573,\n",
       "         'your': 26790,\n",
       "         'name': 2839,\n",
       "         'forget': 1316,\n",
       "         'it': 60008,\n",
       "         'no': 27118,\n",
       "         'its': 24458,\n",
       "         'my': 26249,\n",
       "         'fault': 456,\n",
       "         'didnt': 8043,\n",
       "         'have': 27980,\n",
       "         'a': 89106,\n",
       "         'proper': 126,\n",
       "         'introduction': 19,\n",
       "         'cameron': 34,\n",
       "         'thing': 4976,\n",
       "         'is': 37369,\n",
       "         'im': 30345,\n",
       "         'at': 13283,\n",
       "         'mercy': 62,\n",
       "         'of': 47394,\n",
       "         'particularly': 102,\n",
       "         'hideous': 19,\n",
       "         'breed': 31,\n",
       "         'loser': 113,\n",
       "         'sister': 585,\n",
       "         'cant': 8757,\n",
       "         'date': 507,\n",
       "         'until': 1236,\n",
       "         'she': 11344,\n",
       "         'does': 3336,\n",
       "         'seems': 751,\n",
       "         'like': 19130,\n",
       "         'could': 7311,\n",
       "         'get': 17562,\n",
       "         'easy': 1089,\n",
       "         'enough': 2295,\n",
       "         'why': 12219,\n",
       "         'unsolved': 13,\n",
       "         'mystery': 84,\n",
       "         'used': 1767,\n",
       "         'to': 100667,\n",
       "         'be': 24192,\n",
       "         'really': 6196,\n",
       "         'popular': 94,\n",
       "         'when': 9007,\n",
       "         'started': 709,\n",
       "         'high': 749,\n",
       "         'school': 1358,\n",
       "         'was': 25287,\n",
       "         'just': 20634,\n",
       "         'got': 14743,\n",
       "         'sick': 852,\n",
       "         'or': 7776,\n",
       "         'something': 6827,\n",
       "         'shame': 149,\n",
       "         'gosh': 103,\n",
       "         'only': 5044,\n",
       "         'find': 3549,\n",
       "         'kat': 39,\n",
       "         'boyfriend': 253,\n",
       "         'let': 4589,\n",
       "         'see': 10290,\n",
       "         'what': 44770,\n",
       "         'do': 30486,\n",
       "         'cesc': 1,\n",
       "         'ma': 309,\n",
       "         'tete': 1,\n",
       "         'head': 1438,\n",
       "         'right': 12791,\n",
       "         'ready': 1076,\n",
       "         'for': 29046,\n",
       "         'quiz': 9,\n",
       "         'dont': 33134,\n",
       "         'want': 14744,\n",
       "         'know': 29118,\n",
       "         'say': 8124,\n",
       "         'that': 44755,\n",
       "         'though': 842,\n",
       "         'useful': 66,\n",
       "         'things': 3447,\n",
       "         'where': 7503,\n",
       "         'good': 9378,\n",
       "         'stores': 35,\n",
       "         'much': 4715,\n",
       "         'because': 4218,\n",
       "         'such': 1326,\n",
       "         'nice': 2319,\n",
       "         'one': 12865,\n",
       "         'our': 4763,\n",
       "         'little': 5569,\n",
       "         'wench': 10,\n",
       "         'plan': 611,\n",
       "         'progressing': 3,\n",
       "         'theres': 5085,\n",
       "         'someone': 2121,\n",
       "         'think': 14340,\n",
       "         'might': 2406,\n",
       "         'there': 12922,\n",
       "         'mind': 2249,\n",
       "         'counted': 31,\n",
       "         'help': 3299,\n",
       "         'cause': 1230,\n",
       "         'thug': 7,\n",
       "         'obviously': 224,\n",
       "         'failing': 19,\n",
       "         'arent': 1347,\n",
       "         'ever': 3650,\n",
       "         'going': 11379,\n",
       "         'word': 1164,\n",
       "         'as': 9331,\n",
       "         'gentleman': 153,\n",
       "         'sweet': 442,\n",
       "         'hair': 532,\n",
       "         'look': 7492,\n",
       "         'ebers': 1,\n",
       "         'deep': 314,\n",
       "         'conditioner': 11,\n",
       "         'every': 2226,\n",
       "         'two': 4538,\n",
       "         'days': 1424,\n",
       "         'never': 7067,\n",
       "         'use': 1444,\n",
       "         'blowdryer': 2,\n",
       "         'without': 1488,\n",
       "         'diffuser': 1,\n",
       "         'attachment': 5,\n",
       "         'sure': 6047,\n",
       "         'wanna': 1241,\n",
       "         'go': 12495,\n",
       "         'but': 22020,\n",
       "         'unless': 479,\n",
       "         'goes': 873,\n",
       "         'workin': 92,\n",
       "         'doesnt': 3054,\n",
       "         'seem': 738,\n",
       "         'goin': 604,\n",
       "         'him': 14719,\n",
       "         'shes': 3954,\n",
       "         'lesbian': 28,\n",
       "         'found': 1550,\n",
       "         'picture': 549,\n",
       "         'jared': 3,\n",
       "         'leto': 6,\n",
       "         'in': 41691,\n",
       "         'her': 11430,\n",
       "         'drawers': 15,\n",
       "         'pretty': 1791,\n",
       "         'harboring': 6,\n",
       "         'samesex': 3,\n",
       "         'tendencies': 6,\n",
       "         'kind': 2730,\n",
       "         'guy': 3124,\n",
       "         'likes': 369,\n",
       "         'ones': 702,\n",
       "         'who': 9152,\n",
       "         'knows': 1204,\n",
       "         'all': 18850,\n",
       "         'ive': 7039,\n",
       "         'heard': 1976,\n",
       "         'shed': 281,\n",
       "         'dip': 24,\n",
       "         'before': 3587,\n",
       "         'dating': 86,\n",
       "         'smokes': 38,\n",
       "         'hi': 1204,\n",
       "         'looks': 1232,\n",
       "         'worked': 569,\n",
       "         'tonight': 1819,\n",
       "         'huh': 2286,\n",
       "         'chastity': 12,\n",
       "         'believe': 3360,\n",
       "         'share': 248,\n",
       "         'art': 290,\n",
       "         'instructor': 6,\n",
       "         'fun': 658,\n",
       "         'tons': 51,\n",
       "         'looked': 555,\n",
       "         'back': 7418,\n",
       "         'party': 809,\n",
       "         'always': 3299,\n",
       "         'seemed': 238,\n",
       "         'occupied': 25,\n",
       "         'wanted': 2354,\n",
       "         'did': 11887,\n",
       "         'had': 7015,\n",
       "         'been': 8487,\n",
       "         'selfish': 56,\n",
       "         'guillermo': 1,\n",
       "         'says': 1567,\n",
       "         'any': 5405,\n",
       "         'lighter': 28,\n",
       "         'gonna': 5323,\n",
       "         'extra': 200,\n",
       "         '90210': 1,\n",
       "         'listen': 2434,\n",
       "         'crap': 273,\n",
       "         'endless': 20,\n",
       "         'blonde': 105,\n",
       "         'babble': 7,\n",
       "         'boring': 144,\n",
       "         'myself': 1445,\n",
       "         'thank': 2557,\n",
       "         'god': 2881,\n",
       "         'hear': 2351,\n",
       "         'more': 5498,\n",
       "         'story': 1143,\n",
       "         'about': 18683,\n",
       "         'coiffure': 4,\n",
       "         'figured': 438,\n",
       "         'youd': 1980,\n",
       "         'stuff': 1427,\n",
       "         'eventually': 103,\n",
       "         'real': 2247,\n",
       "         'fear': 302,\n",
       "         'wearing': 334,\n",
       "         'pastels': 2,\n",
       "         'kidding': 587,\n",
       "         'sometimes': 934,\n",
       "         'become': 398,\n",
       "         'persona': 4,\n",
       "         'quit': 461,\n",
       "         'need': 5194,\n",
       "         'learn': 541,\n",
       "         'lie': 600,\n",
       "         'wow': 348,\n",
       "         'lets': 3125,\n",
       "         'hope': 1256,\n",
       "         'they': 15365,\n",
       "         'change': 966,\n",
       "         'he': 24379,\n",
       "         'here': 15848,\n",
       "         'joey': 164,\n",
       "         'great': 2823,\n",
       "         'would': 7915,\n",
       "         'getting': 2127,\n",
       "         'drink': 1073,\n",
       "         'practically': 131,\n",
       "         'proposed': 44,\n",
       "         'same': 1858,\n",
       "         'dermatologist': 3,\n",
       "         'mean': 7182,\n",
       "         'dr': 1071,\n",
       "         'bonchowski': 1,\n",
       "         'hes': 8639,\n",
       "         'exactly': 1380,\n",
       "         'relevant': 23,\n",
       "         'oily': 7,\n",
       "         'dry': 168,\n",
       "         'combination': 58,\n",
       "         'hed': 532,\n",
       "         'different': 1030,\n",
       "         'bianca': 31,\n",
       "         'highlights': 6,\n",
       "         'dorsey': 8,\n",
       "         'include': 53,\n",
       "         'dooropening': 1,\n",
       "         'coatholding': 1,\n",
       "         'wonder': 505,\n",
       "         'guys': 2071,\n",
       "         'were': 14086,\n",
       "         'supposed': 1253,\n",
       "         'actually': 1166,\n",
       "         'id': 3964,\n",
       "         'give': 4358,\n",
       "         'private': 399,\n",
       "         'line': 678,\n",
       "         'home': 2769,\n",
       "         'twenty': 742,\n",
       "         'minutes': 1168,\n",
       "         'til': 167,\n",
       "         're': 64,\n",
       "         'sophomore': 13,\n",
       "         'prom': 101,\n",
       "         'expensive': 110,\n",
       "         'bogey': 11,\n",
       "         'lowenbraus': 1,\n",
       "         'hopefully': 28,\n",
       "         'yeah': 10448,\n",
       "         'sears': 9,\n",
       "         'catalog': 5,\n",
       "         'tube': 37,\n",
       "         'sock': 19,\n",
       "         'gig': 59,\n",
       "         'huge': 126,\n",
       "         'ad': 66,\n",
       "         'queen': 179,\n",
       "         'harry': 596,\n",
       "         'gay': 118,\n",
       "         'cruise': 30,\n",
       "         'ill': 8849,\n",
       "         'uniform': 85,\n",
       "         'neat': 70,\n",
       "         'agent': 391,\n",
       "         'shot': 993,\n",
       "         'being': 1941,\n",
       "         'prada': 4,\n",
       "         'next': 1543,\n",
       "         'year': 1055,\n",
       "         'hey': 3185,\n",
       "         'cheeks': 23,\n",
       "         'concentrating': 16,\n",
       "         'awfully': 71,\n",
       "         'hard': 1384,\n",
       "         'considering': 87,\n",
       "         'gym': 27,\n",
       "         'class': 413,\n",
       "         'talk': 4268,\n",
       "         'deal': 1253,\n",
       "         't': 95,\n",
       "         'whereve': 29,\n",
       "         'nowhere': 175,\n",
       "         'daddy': 696,\n",
       "         'potential': 59,\n",
       "         'smack': 21,\n",
       "         'way': 6112,\n",
       "         'least': 1088,\n",
       "         'bra': 28,\n",
       "         'oh': 10883,\n",
       "         'becoming': 83,\n",
       "         'normal': 256,\n",
       "         'means': 850,\n",
       "         'gigglepuss': 6,\n",
       "         'playing': 546,\n",
       "         'club': 307,\n",
       "         'skunk': 15,\n",
       "         'bothering': 92,\n",
       "         'ask': 2462,\n",
       "         'lowensteins': 3,\n",
       "         'freak': 142,\n",
       "         'torture': 71,\n",
       "         'suck': 108,\n",
       "         'ruining': 35,\n",
       "         'life': 3326,\n",
       "         'wont': 3337,\n",
       "         'too': 6295,\n",
       "         'busy': 449,\n",
       "         'listening': 311,\n",
       "         'bitches': 46,\n",
       "         'prozac': 6,\n",
       "         'completely': 372,\n",
       "         'wretched': 24,\n",
       "         'clouted': 1,\n",
       "         'fen': 1,\n",
       "         'sucked': 44,\n",
       "         'hedgepig': 1,\n",
       "         'even': 3907,\n",
       "         'shakespeare': 53,\n",
       "         'maybe': 5188,\n",
       "         'youve': 3787,\n",
       "         'friend': 1826,\n",
       "         'mandellas': 1,\n",
       "         'guess': 2645,\n",
       "         'since': 1437,\n",
       "         'allowed': 153,\n",
       "         'should': 4563,\n",
       "         'obsess': 1,\n",
       "         'over': 4909,\n",
       "         'dead': 2470,\n",
       "         'unbalanced': 2,\n",
       "         'now': 12037,\n",
       "         'tell': 8991,\n",
       "         'social': 127,\n",
       "         'advice': 235,\n",
       "         'from': 8496,\n",
       "         'act': 503,\n",
       "         'totally': 322,\n",
       "         'apeshit': 3,\n",
       "         'welcome': 341,\n",
       "         'hate': 1041,\n",
       "         'sit': 1044,\n",
       "         'susie': 79,\n",
       "         'care': 2058,\n",
       "         'firm': 109,\n",
       "         'believer': 21,\n",
       "         'doing': 4013,\n",
       "         'own': 1961,\n",
       "         'reasons': 172,\n",
       "         'else': 2336,\n",
       "         's': 97,\n",
       "         'wish': 1030,\n",
       "         'luxury': 19,\n",
       "         'asked': 1091,\n",
       "         'won': 261,\n",
       "         'told': 3710,\n",
       "         'went': 1891,\n",
       "         '9th': 14,\n",
       "         'month': 403,\n",
       "         'total': 165,\n",
       "         'babe': 146,\n",
       "         'said': 5404,\n",
       "         'everyone': 811,\n",
       "         'once': 1540,\n",
       "         'afterwards': 50,\n",
       "         'anymore': 906,\n",
       "         'wasnt': 2242,\n",
       "         'pissed': 151,\n",
       "         'broke': 405,\n",
       "         'after': 3015,\n",
       "         'swore': 62,\n",
       "         'anything': 4396,\n",
       "         'havent': 1917,\n",
       "         'except': 531,\n",
       "         'bogeys': 3,\n",
       "         'decisions': 48,\n",
       "         'wouldve': 190,\n",
       "         'instead': 320,\n",
       "         'helping': 169,\n",
       "         'stupid': 768,\n",
       "         'repeat': 123,\n",
       "         'mistakes': 80,\n",
       "         'protecting': 81,\n",
       "         'keep': 2831,\n",
       "         'locked': 224,\n",
       "         'away': 2955,\n",
       "         'dark': 471,\n",
       "         'experience': 273,\n",
       "         'experiences': 33,\n",
       "         'trust': 967,\n",
       "         'people': 5027,\n",
       "         'will': 7900,\n",
       "         'beautiful': 1043,\n",
       "         'last': 3386,\n",
       "         'set': 921,\n",
       "         'damage': 148,\n",
       "         'send': 800,\n",
       "         'therapy': 86,\n",
       "         'forever': 263,\n",
       "         'woman': 1418,\n",
       "         'complete': 190,\n",
       "         'fruitloop': 1,\n",
       "         'patrick': 119,\n",
       "         'perm': 8,\n",
       "         'upset': 367,\n",
       "         'boy': 1874,\n",
       "         'starts': 160,\n",
       "         'end': 969,\n",
       "         'discussion': 64,\n",
       "         'neither': 346,\n",
       "         'sleep': 1090,\n",
       "         'fair': 404,\n",
       "         'mutant': 27,\n",
       "         'point': 1305,\n",
       "         'wherere': 43,\n",
       "         'must': 3306,\n",
       "         'attempting': 21,\n",
       "         'small': 559,\n",
       "         'study': 215,\n",
       "         'group': 238,\n",
       "         'friends': 1322,\n",
       "         'otherwise': 187,\n",
       "         'known': 545,\n",
       "         'orgy': 11,\n",
       "         'knew': 1951,\n",
       "         'forbid': 21,\n",
       "         'gloria': 30,\n",
       "         'steinem': 3,\n",
       "         'isnt': 3129,\n",
       "         'expect': 571,\n",
       "         'kats': 1,\n",
       "         'starting': 286,\n",
       "         'wear': 443,\n",
       "         'belly': 46,\n",
       "         'minute': 1346,\n",
       "         'promise': 638,\n",
       "         'boys': 743,\n",
       "         'present': 292,\n",
       "         'shell': 495,\n",
       "         'scare': 185,\n",
       "         'them': 7208,\n",
       "         'discuss': 213,\n",
       "         'tomorrow': 1583,\n",
       "         'has': 4569,\n",
       "         'hot': 632,\n",
       "         'rod': 64,\n",
       "         'bend': 53,\n",
       "         'rules': 240,\n",
       "         'whatever': 1066,\n",
       "         'fine': 2471,\n",
       "         'prisoner': 67,\n",
       "         'house': 2089,\n",
       "         'daughter': 545,\n",
       "         'possession': 79,\n",
       "         'missing': 323,\n",
       "         'captain': 862,\n",
       "         'oppression': 7,\n",
       "         'men': 1467,\n",
       "         'pleasure': 324,\n",
       "         'brucie': 1,\n",
       "         'pegged': 19,\n",
       "         'fan': 136,\n",
       "         'preteen': 3,\n",
       "         'bellybutton': 3,\n",
       "         'ring': 323,\n",
       "         'couple': 981,\n",
       "         'minors': 12,\n",
       "         'come': 8509,\n",
       "         'padua': 2,\n",
       "         'girls': 720,\n",
       "         'tall': 109,\n",
       "         'decent': 116,\n",
       "         'body': 720,\n",
       "         'other': 3340,\n",
       "         'kinda': 419,\n",
       "         'short': 318,\n",
       "         'undersexed': 2,\n",
       "         'sent': 673,\n",
       "         'em': 1620,\n",
       "         'through': 2100,\n",
       "         'new': 2741,\n",
       "         'cmon': 536,\n",
       "         'tour': 93,\n",
       "         'which': 1660,\n",
       "         'dakota': 20,\n",
       "         'north': 222,\n",
       "         'howd': 488,\n",
       "         'live': 1690,\n",
       "         'outnumbered': 6,\n",
       "         'by': 4866,\n",
       "         'cows': 36,\n",
       "         'many': 1648,\n",
       "         'old': 2969,\n",
       "         'thirtytwo': 22,\n",
       "         'thousand': 1057,\n",
       "         'most': 1452,\n",
       "         'evil': 307,\n",
       "         'these': 3406,\n",
       "         'seen': 1863,\n",
       "         'horse': 271,\n",
       "         'jack': 1026,\n",
       "         'off': 4779,\n",
       "         'clint': 8,\n",
       "         'eastwood': 6,\n",
       "         'girl': 2391,\n",
       "         'burn': 172,\n",
       "         'pine': 20,\n",
       "         'perish': 10,\n",
       "         'stratford': 10,\n",
       "         'haircut': 39,\n",
       "         'matter': 1675,\n",
       "         'older': 176,\n",
       "         'impossibility': 4,\n",
       "         'theyre': 3890,\n",
       "         'bred': 17,\n",
       "         'their': 2501,\n",
       "         'mothers': 284,\n",
       "         'liked': 424,\n",
       "         'grandmothers': 18,\n",
       "         'gene': 35,\n",
       "         'pool': 177,\n",
       "         'rarely': 31,\n",
       "         'diluted': 2,\n",
       "         'shiteating': 3,\n",
       "         'grin': 4,\n",
       "         'permashitgrin': 1,\n",
       "         'moron': 68,\n",
       "         'number': 826,\n",
       "         'twelve': 384,\n",
       "         'model': 94,\n",
       "         'mostly': 193,\n",
       "         'regional': 14,\n",
       "         'moms': 105,\n",
       "         'canada': 43,\n",
       "         'signed': 128,\n",
       "         'tutor': 13,\n",
       "         'chance': 984,\n",
       "         'consecrate': 1,\n",
       "         'minor': 73,\n",
       "         'encounter': 22,\n",
       "         'shrew': 1,\n",
       "         'biancas': 3,\n",
       "         'mewling': 1,\n",
       "         'rampalian': 1,\n",
       "         'wretch': 5,\n",
       "         'herself': 213,\n",
       "         'teach': 272,\n",
       "         'dazzle': 3,\n",
       "         'charm': 54,\n",
       "         'falls': 106,\n",
       "         'love': 3993,\n",
       "         'unlikely': 27,\n",
       "         'still': 3648,\n",
       "         'makes': 1209,\n",
       "         'hell': 3710,\n",
       "         'thrives': 2,\n",
       "         'danger': 207,\n",
       "         'criminal': 169,\n",
       "         'lit': 37,\n",
       "         'state': 477,\n",
       "         'trooper': 17,\n",
       "         'fire': 742,\n",
       "         'alcatraz': 2,\n",
       "         'felons': 3,\n",
       "         'honors': 18,\n",
       "         'biology': 12,\n",
       "         'serious': 785,\n",
       "         'man': 6906,\n",
       "         'whacked': 31,\n",
       "         'sold': 220,\n",
       "         'his': 8551,\n",
       "         'liver': 25,\n",
       "         'black': 737,\n",
       "         'market': 145,\n",
       "         'buy': 827,\n",
       "         'speakers': 6,\n",
       "         'reputation': 115,\n",
       "         'weve': 1785,\n",
       "         'outrank': 2,\n",
       "         'strictly': 51,\n",
       "         'alist': 2,\n",
       "         'side': 827,\n",
       "         'hated': 162,\n",
       "         'those': 3210,\n",
       "         'gotta': 2246,\n",
       "         'few': 1451,\n",
       "         'clients': 84,\n",
       "         'wall': 249,\n",
       "         'street': 673,\n",
       "         'involved': 378,\n",
       "         'choice': 452,\n",
       "         'besides': 515,\n",
       "         'enemy': 187,\n",
       "         'orchestrating': 1,\n",
       "         'battle': 110,\n",
       "         'position': 261,\n",
       "         'power': 676,\n",
       "         'golden': 65,\n",
       "         'opportunity': 143,\n",
       "         'katarina': 3,\n",
       "         'case': 1278,\n",
       "         'schoolwide': 2,\n",
       "         'blow': 435,\n",
       "         'bent': 48,\n",
       "         'piss': 125,\n",
       "         'himself': 645,\n",
       "         'joy': 66,\n",
       "         'ultimate': 39,\n",
       "         'kiss': 364,\n",
       "         'ass': 1043,\n",
       "         'hates': 137,\n",
       "         'smokers': 4,\n",
       "         'lung': 33,\n",
       "         'cancer': 134,\n",
       "         'issue': 139,\n",
       "         'favorite': 234,\n",
       "         'uncle': 360,\n",
       "         'fortyone': 11,\n",
       "         'assail': 1,\n",
       "         'ears': 150,\n",
       "         'band': 171,\n",
       "         'already': 1511,\n",
       "         'whole': 1689,\n",
       "         'extremely': 102,\n",
       "         'unfortunate': 62,\n",
       "         'maneuver': 17,\n",
       "         'picks': 37,\n",
       "         'carries': 22,\n",
       "         'while': 1445,\n",
       "         'talking': 2690,\n",
       "         'buttholus': 2,\n",
       "         'extremus': 2,\n",
       "         'making': 960,\n",
       "         'progress': 96,\n",
       "         'm': 96,\n",
       "         'humiliated': 24,\n",
       "         'sacrifice': 54,\n",
       "         'yourself': 2210,\n",
       "         'altar': 10,\n",
       "         'dignity': 38,\n",
       "         'score': 116,\n",
       "         'best': 1669,\n",
       "         'scenario': 21,\n",
       "         'payroll': 30,\n",
       "         'awhile': 169,\n",
       "         'non': 36,\n",
       "         'prisonmovie': 1,\n",
       "         'type': 267,\n",
       "         'whatve': 25,\n",
       "         'retrieved': 4,\n",
       "         'certain': 395,\n",
       "         'pieces': 147,\n",
       "         'information': 469,\n",
       "         'miss': 1691,\n",
       "         'youll': 2796,\n",
       "         'helpful': 53,\n",
       "         'thai': 10,\n",
       "         'food': 536,\n",
       "         'feminist': 7,\n",
       "         'prose': 6,\n",
       "         'angry': 251,\n",
       "         'stinky': 10,\n",
       "         'music': 438,\n",
       "         'indierock': 1,\n",
       "         'persuasion': 4,\n",
       "         'noodles': 7,\n",
       "         'book': 754,\n",
       "         'around': 3391,\n",
       "         'chicks': 73,\n",
       "         'play': 1423,\n",
       "         'partial': 29,\n",
       "         'whatd': 471,\n",
       "         'don': 198,\n",
       "         'decided': 325,\n",
       "         'nail': 91,\n",
       "         'drunk': 384,\n",
       "         'remember': 2678,\n",
       "         'suns': 25,\n",
       "         'direct': 123,\n",
       "         'quote': 63,\n",
       "         'needs': 606,\n",
       "         'time': 8172,\n",
       "         'cool': 625,\n",
       "         'day': 2880,\n",
       "         'makin': 91,\n",
       "         'headway': 3,\n",
       "         'kissed': 76,\n",
       "         'worst': 227,\n",
       "         'vintage': 8,\n",
       "         'reading': 285,\n",
       "         'sassy': 11,\n",
       "         'noticed': 208,\n",
       "         'featured': 3,\n",
       "         'big': 2744,\n",
       "         'kmart': 7,\n",
       "         'spread': 74,\n",
       "         'elbow': 13,\n",
       "         'tough': 398,\n",
       "         'running': 758,\n",
       "         'rest': 905,\n",
       "         'ya': 1678,\n",
       "         'leave': 2485,\n",
       "         'alone': 1362,\n",
       "         'legs': 185,\n",
       "         'rack': 23,\n",
       "         'sparky': 10,\n",
       "         'money': 3437,\n",
       "         'take': 7002,\n",
       "         'cake': 107,\n",
       "         'verona': 19,\n",
       "         'pick': 779,\n",
       "         'tab': 27,\n",
       "         'pay': 1074,\n",
       "         'gets': 993,\n",
       "         'catch': 516,\n",
       "         'bucks': 397,\n",
       "         'thirty': 463,\n",
       "         'negotiation': 16,\n",
       "         'fifty': 569,\n",
       "         'results': 56,\n",
       "         'watching': 402,\n",
       "         'bitch': 504,\n",
       "         'trash': 61,\n",
       "         'car': 1948,\n",
       "         'count': 350,\n",
       "         'under': 1068,\n",
       "         'control': 522,\n",
       "         'acts': 41,\n",
       "         'crazed': 8,\n",
       "         'image': 91,\n",
       "         'upped': 9,\n",
       "         'price': 285,\n",
       "         'hundred': 1392,\n",
       "         'deals': 82,\n",
       "         'human': 509,\n",
       "         'limothe': 1,\n",
       "         'flowers': 155,\n",
       "         'another': 2060,\n",
       "         'tux': 6,\n",
       "         'barbie': 14,\n",
       "         'n': 109,\n",
       "         'ken': 36,\n",
       "         'shit': 2940,\n",
       "         'lost': 1133,\n",
       "         'nope': 229,\n",
       "         'came': 2074,\n",
       "         'chat': 82,\n",
       "         'run': 1496,\n",
       "         'idea': 1708,\n",
       "         'interested': 521,\n",
       "         'insane': 222,\n",
       "         'conversation': 192,\n",
       "         'purpose': 162,\n",
       "         'recruit': 15,\n",
       "         'wholl': 41,\n",
       "         'whos': 1478,\n",
       "         'job': 1985,\n",
       "         'helpin': 9,\n",
       "         'uh': 2229,\n",
       "         'took': 1378,\n",
       "         'bathes': 3,\n",
       "         'together': 1390,\n",
       "         'kids': 1143,\n",
       "         'better': 3434,\n",
       "         'fuck': 2598,\n",
       "         'heavily': 24,\n",
       "         'invested': 19,\n",
       "         'higher': 91,\n",
       "         'random': 56,\n",
       "         'skid': 8,\n",
       "         'pat': 56,\n",
       "         'gone': 1303,\n",
       "         'porn': 25,\n",
       "         'movies': 309,\n",
       "         'incapable': 24,\n",
       "         'interesting': 417,\n",
       "         'block': 127,\n",
       "         'e': 63,\n",
       "         'mandella': 1,\n",
       "         'eat': 954,\n",
       "         'starving': 49,\n",
       "         'very': 5095,\n",
       "         'slow': 315,\n",
       "         'die': 1170,\n",
       "         'attempted': 22,\n",
       "         'slit': 27,\n",
       "         'realize': 369,\n",
       "         'institution': 57,\n",
       "         'severely': 10,\n",
       "         'lacking': 23,\n",
       "         'killing': 380,\n",
       "         'william': 137,\n",
       "         'beyond': 196,\n",
       "         'imagine': 405,\n",
       "         'during': 252,\n",
       "         'sex': 602,\n",
       "         'foul': 38,\n",
       "         'raging': 8,\n",
       "         'fit': 247,\n",
       "         'sarah': 121,\n",
       "         'lawrence': 19,\n",
       "         'insists': 17,\n",
       "         'maledominated': 1,\n",
       "         'puking': 7,\n",
       "         'frat': 9,\n",
       "         'golf': 67,\n",
       "         'team': 327,\n",
       "         'proven': 33,\n",
       "         'heterosexuality': 2,\n",
       "         'appreciate': 353,\n",
       "         'efforts': 25,\n",
       "         'toward': 114,\n",
       "         'speedy': 3,\n",
       "         'death': 928,\n",
       "         'consuming': 7,\n",
       "         'precious': 71,\n",
       "         'tiara': 2,\n",
       "         'thisll': 54,\n",
       "         'work': 3572,\n",
       "         'cares': 173,\n",
       "         'officially': 45,\n",
       "         'opposed': 41,\n",
       "         'suburban': 8,\n",
       "         'activity': 42,\n",
       "         'wheres': 1178,\n",
       "         'done': 2170,\n",
       "         'favor': 402,\n",
       "         'backfired': 2,\n",
       "         'puked': 3,\n",
       "         'rejected': 18,\n",
       "         'bastion': 4,\n",
       "         'commercial': 57,\n",
       "         'excess': 8,\n",
       "         'dates': 67,\n",
       "         'sound': 643,\n",
       "         'betty': 244,\n",
       "         'archie': 24,\n",
       "         'taking': 1002,\n",
       "         'veronica': 87,\n",
       "         'dress': 309,\n",
       "         'anyway': 1300,\n",
       "         'looking': 1803,\n",
       "         'wrong': 2488,\n",
       "         'perspective': 31,\n",
       "         'statement': 119,\n",
       "         'us': 7005,\n",
       "         'meet': 1365,\n",
       "         'honey': 909,\n",
       "         'progressed': 3,\n",
       "         'fullon': 4,\n",
       "         'hallucinations': 15,\n",
       "         'doin': 489,\n",
       "         'sweating': 32,\n",
       "         'pig': 156,\n",
       "         'attention': 267,\n",
       "         'mission': 254,\n",
       "         'friday': 168,\n",
       "         'places': 237,\n",
       "         '7eleven': 4,\n",
       "         'burnside': 2,\n",
       "         'screwboy': 2,\n",
       "         'lot': 2521,\n",
       "         'than': 3281,\n",
       "         'warrant': 80,\n",
       "         'strong': 312,\n",
       "         'emotion': 35,\n",
       "         'spend': 343,\n",
       "         'dollar': 236,\n",
       "         'track': 192,\n",
       "         'ponies': 18,\n",
       "         'flat': 121,\n",
       "         'beer': 296,\n",
       "         'eyes': 828,\n",
       "         'hand': 891,\n",
       "         'covered': 124,\n",
       "         'vomit': 35,\n",
       "         'seventhirty': 12,\n",
       "         'following': 203,\n",
       "         'laundromat': 8,\n",
       "         'saw': 2020,\n",
       "         'talker': 10,\n",
       "         'depends': 183,\n",
       "         'topic': 22,\n",
       "         'fenders': 2,\n",
       "         'whip': 41,\n",
       "         'into': 3618,\n",
       "         'verbal': 25,\n",
       "         'frenzy': 6,\n",
       "         'show': 1723,\n",
       "         'excuse': 845,\n",
       "         'sort': 825,\n",
       "         'bikini': 10,\n",
       "         'kill': 2499,\n",
       "         'raincoats': 4,\n",
       "         'trashed': 14,\n",
       "         'funny': 933,\n",
       "         'down': 5401,\n",
       "         'concussion': 15,\n",
       "         'dog': 602,\n",
       "         'woke': 123,\n",
       "         'vegetable': 24,\n",
       "         'patronizing': 10,\n",
       "         'words': 568,\n",
       "         'shitfaced': 7,\n",
       "         'affection': 25,\n",
       "         'blind': 205,\n",
       "         'hatred': 23,\n",
       "         'whyd': 194,\n",
       "         'itd': 158,\n",
       "         'mainline': 2,\n",
       "         'tequila': 20,\n",
       "         'laid': 171,\n",
       "         'above': 156,\n",
       "         'wake': 360,\n",
       "         'jail': 336,\n",
       "         'werent': 675,\n",
       "         'father': 2055,\n",
       "         ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])\n",
    "\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_freq = 5\n",
    "words = [word for word in word_freq.keys() if (word_freq[word] > min_word_freq)]\n",
    "word_map = {k: v+1 for v, k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map)+1\n",
    "word_map['<start>'] = len(word_map)+1\n",
    "word_map['<end>'] = len(word_map)+1\n",
    "word_map['<pad>'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 18243\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words are: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/WORDMAP_corpus.json', 'w') as j:\n",
    "    json.dump(word_map, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(words, word_map):\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']]*(max_len-len(words))\n",
    "    # enc_c = torch.LongTensor(enc_c)\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reply(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']]*(max_len-len(words))\n",
    "    # enc_c = torch.LongTensor(enc_c)\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode_question(pairs[0][0], word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    question = encode_question(pair[0], word_map)\n",
    "    reply = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append([question, reply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pairs_encoded.json', 'w') as w:\n",
    "    json.dump(pairs_encoded, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.pairs = json.load(open('data/pairs_encoded.json', 'r'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        question = torch.LongTensor(self.pairs[index][0])\n",
    "        reply = torch.LongTensor(self.pairs[index][1])\n",
    "        return question, reply\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
    "                                            batch_size=100,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, reply = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "\n",
    "    def subsequent_mask(size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = question!=0\n",
    "    question_mask = question_mask.to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1) # (batch_size, 1, 1, max_words)\n",
    "\n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) # (batch_size, max_words, max_words)\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
    "    reply_target_mask = reply_target!=0\n",
    "\n",
    "    return question_mask, reply_input_mask, reply_target_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How subsequent_mask works\n",
    "# size = 5\n",
    "# t = torch.ones(size, size)\n",
    "# t_triu = torch.triu(t)\n",
    "# t_triu.T # transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question[0] !=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positional_encoding(max_len, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def create_positional_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len): # for each position of the word in the sentence\n",
    "            for i in range(0, d_model, 2): # for each dimension of each position in the word embedding\n",
    "                pe[pos, i] = math.sin(pos/(10000**((2*i)/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos/(10000**((2*(i+1))/d_model)))\n",
    "        pe = pe.unsqueeze(0) # include batch dimension (1, max_len, d_model)\n",
    "        return pe\n",
    "\n",
    "    # include forward function when using nn.Module    \n",
    "    def forward(self, encoded_words):\n",
    "        embeddings = self.embed(encoded_words) * math.sqrt(self.d_model) # (batch_size, max_words, d_model)\n",
    "        # max_words = embeddings.size(1)\n",
    "        embeddings += self.pe[:, :embeddings.size(1)] # pe will automatically be expanded to match the batch size of embeddings matrix (1, max_words, d_model)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\" \n",
    "        query: (batch_size, max_words, d_model); d_model = 512\n",
    "        key: (batch_size, max_words, d_model)\n",
    "        value: (batch_size, max_words, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # linear layer\n",
    "        query = self.query(query) # (batch_size, max_words, d_model)\n",
    "        key = self.key(key)       # (batch_size, max_words, d_model)\n",
    "        value = self.value(value) # (batch_size, max_words, d_model)\n",
    "        \n",
    "        # split into heads\n",
    "        # (batch_size, max_words, d_model) -> (batch_size, max_words, heads, d_k) -> (batch_size, heads, max_words, d_k)\n",
    "        query = query.view(batch_size, -1, self.heads, self.d_k)\n",
    "        key = key.view(batch_size, -1, self.heads, self.d_k)\n",
    "        value = value.view(batch_size, -1, self.heads, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions (batch_size, heads, max_words, d_k)\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "        \n",
    "        # (batch_size, heads, max_words, d_k) dot (batch_size, heads, d_k, max_words) -> (batch_size, heads, max_words, max_words)\n",
    "\n",
    "        # calculate scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        scores = scores.masked_fill(mask==0, -1e9) # (batch_size, heads, max_words, max_words)\n",
    "        weights = F.softmax(scores, dim=-1) # (batch_size, heads, max_words, max_words)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # (batch_size, heads, max_words, max_words) dot (batch_size, heads, max_words, d_k) -> (batch_size, heads, max_words, d_k)\n",
    "        context = torch.matmul(weights, value) # (batch_size, heads, max_words, d_k)\n",
    "\n",
    "        # transpose to get dimensions (batch_size, max_words, heads, d_k)\n",
    "        # combine last two dimensions to concatenate all heads together\n",
    "        # (batch_size, 8, max_words, 64) -> (batch_size, max_words, 8, d_k) (batch_size, max_words, 8*64)\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.heads*self.d_k)\n",
    "        output = self.concat(context) # (batch_size, max_words, d_model)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # showing how mask works\n",
    "# a = torch.randn(2, 2)\n",
    "# mask = torch.tensor([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.masked_fill(mask==0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        self_attn = self.attn(embeddings, embeddings, embeddings, mask) # (query, key, value, mask)\n",
    "        self_attn = self.dropout(self_attn)\n",
    "        self_attn = self.layer_norm(self_attn + embeddings)\n",
    "        ff = self.ff(self_attn)\n",
    "        ff = self.dropout(ff)\n",
    "        encoder_out = self.layer_norm(self_attn + ff)\n",
    "        return encoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, encoder_out, src_mask, target_mask):\n",
    "        query = self.attn(embeddings, embeddings, embeddings, target_mask) # (query, key, value, mask)\n",
    "        query = self.dropout(query)\n",
    "        query = self.layer_norm(query + embeddings)\n",
    "        src_attn = self.attn(query, encoder_out, encoder_out, src_mask) # (query, key, value, mask)\n",
    "        src_attn = self.dropout(src_attn)\n",
    "        src_attn = self.layer_norm(src_attn + query)\n",
    "        ff = self.ff(src_attn)\n",
    "        ff = self.dropout(ff)\n",
    "        decoder_out = self.layer_norm(src_attn + ff)\n",
    "        return decoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "\n",
    "    def encode(self, src_words, src_mask): # src_words aka question\n",
    "        src_embeddings = self.embed(src_words)\n",
    "        for layer in self.encoder:\n",
    "            src_embeddings = layer(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "\n",
    "    def decode(self, tgt_words, tgt_mask, src_embedding, src_mask, ): # tgt_words aka reply\n",
    "        tgt_embeddings = self.embed(tgt_words)\n",
    "        for layer in self.decoder:\n",
    "            tgt_embeddings = layer(tgt_embeddings, src_embedding, src_mask, tgt_mask)\n",
    "        return tgt_embeddings\n",
    "    \n",
    "    def forward(self, src_words, src_mask, tgt_words, tgt_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(tgt_words, tgt_mask, encoded, src_mask)\n",
    "        logits = self.logit(decoded)\n",
    "        out = F.log_softmax(logits, dim=-1) # include manually because KLDivLoss requires log_softmax unlike cross_entropy\n",
    "        return out\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "\n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.model_size**(-0.5) * min(self.current_step**(-0.5), self.current_step*self.warmup_steps**(-1.5))\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.lr = lr\n",
    "        # update weights\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "    def __init__(self, size, smoothing):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\" \n",
    "        prediction: (batch_size, max_words, vocab_size)\n",
    "        target: (batch_size, max_words)\n",
    "        mask: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1)) # (batch_size*max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1) # (batch_size*max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1) # (batch_size*max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smoothing / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)             # (batch_size*max_words, vocab_size)\n",
    "        loss = (loss.sum(dim=1) * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 3\n",
    "# max_words = 5\n",
    "# vocab_size = 7\n",
    "# loss = torch.randn(batch_size * max_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = torch.randn(15)\n",
    "# mask.shape\n",
    "# mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (loss.sum(1) * mask.float()).sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 3\n",
    "# max_words = 5\n",
    "# vocab_size = 7\n",
    "# prediction = torch.randn(batch_size, max_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = prediction.view(-1, prediction.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.LongTensor(batch_size * max_words).random_(0, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = target!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = prediction.data.clone()\n",
    "# labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.fill_(0.3 / 3 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.scatter(1, target.data.unsqueeze(1), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\nn-bc\\venv\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Define Model, Optimizer, Loss\n",
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "TORCH_USE_CUDA_DSA=1\n",
    "with open('data/WORDMAP_corpus.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "transformer = Transformer(d_model=d_model, heads = heads, num_layers = num_layers, word_map = word_map).to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(d_model, 4000, adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    transformer.train()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        samples = question.shape[0]\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "        \n",
    "        # Sentence: <start> I went home . <end>\n",
    "        # reply_input: <start> I went home .\n",
    "        # reply_target: I went home . <end>\n",
    "\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "\n",
    "        # create masks and add dimensions\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "\n",
    "        # Run the Transformer model and get outputs\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "\n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch {} | Batch {}/{} | Loss {}\".format(epoch+1, i, len(train_loader), total_loss/count))\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['<start>']\n",
    "\n",
    "    # Encode\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device) # (batch_size, 1)\n",
    "\n",
    "    # Decoding starts here\n",
    "    for step in range(max_len-1):\n",
    "        size = words.shape[0]\n",
    "\n",
    "        # create target mask\n",
    "        tgt_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        tgt_mask = tgt_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # decode\n",
    "        # decoded shape: (batch_size, max_words, d_model) = (1, 1, vocab_size)\n",
    "        decoded = transformer.decode(words, tgt_mask, encoded, question_mask)\n",
    "\n",
    "        # predict next word using logits and softmax\n",
    "        # predictions shape: (max_words, vocab_size) = (1, vocab_size)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim=1) #(1, 1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "    words = torch.cat([words, torch.LongTensor([[next_word]].to(device))], dim=1) #(1, step+2)\n",
    "\n",
    "    #(1,5) -> (5)\n",
    "    words = words.squeeze(0)  # (1, step+1) -> (step+1)\n",
    "    words = words.tolist()\n",
    "\n",
    "    # concat words to form sentence\n",
    "    sentence_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sentence_idx[k]] for k in range(len(sentence_idx))])\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0/2217 | Loss 4.394916534423828\n",
      "Epoch 1 | Batch 100/2217 | Loss 4.433915964447626\n",
      "Epoch 1 | Batch 200/2217 | Loss 4.4352311305145715\n",
      "Epoch 1 | Batch 300/2217 | Loss 4.42975094310469\n",
      "Epoch 1 | Batch 400/2217 | Loss 4.428952117215963\n",
      "Epoch 1 | Batch 500/2217 | Loss 4.4260578326836315\n",
      "Epoch 1 | Batch 600/2217 | Loss 4.421902313010268\n",
      "Epoch 1 | Batch 700/2217 | Loss 4.4195305080114515\n",
      "Epoch 1 | Batch 800/2217 | Loss 4.418576418534944\n",
      "Epoch 1 | Batch 900/2217 | Loss 4.418793742849877\n",
      "Epoch 1 | Batch 1000/2217 | Loss 4.418258470731539\n",
      "Epoch 1 | Batch 1100/2217 | Loss 4.417297190909598\n",
      "Epoch 1 | Batch 1200/2217 | Loss 4.415683094408987\n",
      "Epoch 1 | Batch 1300/2217 | Loss 4.414350040136714\n",
      "Epoch 1 | Batch 1400/2217 | Loss 4.412398444508587\n",
      "Epoch 1 | Batch 1500/2217 | Loss 4.409784390082922\n",
      "Epoch 1 | Batch 1600/2217 | Loss 4.408918334870395\n",
      "Epoch 1 | Batch 1700/2217 | Loss 4.40789369384658\n",
      "Epoch 1 | Batch 1800/2217 | Loss 4.4075559116217375\n",
      "Epoch 1 | Batch 1900/2217 | Loss 4.40605611941615\n",
      "Epoch 1 | Batch 2000/2217 | Loss 4.404111919851079\n",
      "Epoch 1 | Batch 2100/2217 | Loss 4.40374620819818\n",
      "Epoch 1 | Batch 2200/2217 | Loss 4.402565508309086\n",
      "Epoch 2 | Batch 0/2217 | Loss 4.442556381225586\n",
      "Epoch 2 | Batch 100/2217 | Loss 4.312951824452617\n",
      "Epoch 2 | Batch 200/2217 | Loss 4.313968105695734\n",
      "Epoch 2 | Batch 300/2217 | Loss 4.311202177573676\n",
      "Epoch 2 | Batch 400/2217 | Loss 4.309957551837265\n",
      "Epoch 2 | Batch 500/2217 | Loss 4.305796312952708\n",
      "Epoch 2 | Batch 600/2217 | Loss 4.304887117046286\n",
      "Epoch 2 | Batch 700/2217 | Loss 4.304574702163566\n",
      "Epoch 2 | Batch 800/2217 | Loss 4.30385873826702\n",
      "Epoch 2 | Batch 900/2217 | Loss 4.30278191238344\n",
      "Epoch 2 | Batch 1000/2217 | Loss 4.300753104460465\n",
      "Epoch 2 | Batch 1100/2217 | Loss 4.298724868533613\n",
      "Epoch 2 | Batch 1200/2217 | Loss 4.297537869358936\n",
      "Epoch 2 | Batch 1300/2217 | Loss 4.296762563007598\n",
      "Epoch 2 | Batch 1400/2217 | Loss 4.2966264420794555\n",
      "Epoch 2 | Batch 1500/2217 | Loss 4.297149589107801\n",
      "Epoch 2 | Batch 1600/2217 | Loss 4.296731442678429\n",
      "Epoch 2 | Batch 1700/2217 | Loss 4.296004229612311\n",
      "Epoch 2 | Batch 1800/2217 | Loss 4.294498553347548\n",
      "Epoch 2 | Batch 1900/2217 | Loss 4.293578377778376\n",
      "Epoch 2 | Batch 2000/2217 | Loss 4.292903225163351\n",
      "Epoch 2 | Batch 2100/2217 | Loss 4.2912795021669\n",
      "Epoch 2 | Batch 2200/2217 | Loss 4.290230168152376\n",
      "Epoch 3 | Batch 0/2217 | Loss 4.213217258453369\n",
      "Epoch 3 | Batch 100/2217 | Loss 4.167426102232225\n",
      "Epoch 3 | Batch 200/2217 | Loss 4.168654657714995\n",
      "Epoch 3 | Batch 300/2217 | Loss 4.175696532987677\n",
      "Epoch 3 | Batch 400/2217 | Loss 4.177178257421365\n",
      "Epoch 3 | Batch 500/2217 | Loss 4.181703768804402\n",
      "Epoch 3 | Batch 600/2217 | Loss 4.183069014112882\n",
      "Epoch 3 | Batch 700/2217 | Loss 4.182541079596004\n",
      "Epoch 3 | Batch 800/2217 | Loss 4.1828134741527165\n",
      "Epoch 3 | Batch 900/2217 | Loss 4.183226336384984\n",
      "Epoch 3 | Batch 1000/2217 | Loss 4.1839685306682455\n",
      "Epoch 3 | Batch 1100/2217 | Loss 4.185697555541992\n",
      "Epoch 3 | Batch 1200/2217 | Loss 4.185012768150666\n",
      "Epoch 3 | Batch 1300/2217 | Loss 4.186825061009354\n",
      "Epoch 3 | Batch 1400/2217 | Loss 4.187705759488201\n",
      "Epoch 3 | Batch 1500/2217 | Loss 4.189003486779433\n",
      "Epoch 3 | Batch 1600/2217 | Loss 4.189895248353518\n",
      "Epoch 3 | Batch 1700/2217 | Loss 4.190009370963059\n",
      "Epoch 3 | Batch 1800/2217 | Loss 4.1900376047709464\n",
      "Epoch 3 | Batch 1900/2217 | Loss 4.1901404290748605\n",
      "Epoch 3 | Batch 2000/2217 | Loss 4.191102459095884\n",
      "Epoch 3 | Batch 2100/2217 | Loss 4.192061798621336\n",
      "Epoch 3 | Batch 2200/2217 | Loss 4.192390521728467\n",
      "Epoch 4 | Batch 0/2217 | Loss 4.230414390563965\n",
      "Epoch 4 | Batch 100/2217 | Loss 4.088999490926762\n",
      "Epoch 4 | Batch 200/2217 | Loss 4.093525719286791\n",
      "Epoch 4 | Batch 300/2217 | Loss 4.0968559113055765\n",
      "Epoch 4 | Batch 400/2217 | Loss 4.097766779307415\n",
      "Epoch 4 | Batch 500/2217 | Loss 4.0988983746298295\n",
      "Epoch 4 | Batch 600/2217 | Loss 4.10218141717641\n",
      "Epoch 4 | Batch 700/2217 | Loss 4.1028895660405835\n",
      "Epoch 4 | Batch 800/2217 | Loss 4.104695817504483\n",
      "Epoch 4 | Batch 900/2217 | Loss 4.106418483926771\n",
      "Epoch 4 | Batch 1000/2217 | Loss 4.108389079392135\n",
      "Epoch 4 | Batch 1100/2217 | Loss 4.108743862711658\n",
      "Epoch 4 | Batch 1200/2217 | Loss 4.109784283308462\n",
      "Epoch 4 | Batch 1300/2217 | Loss 4.111164820551597\n",
      "Epoch 4 | Batch 1400/2217 | Loss 4.112519827848839\n",
      "Epoch 4 | Batch 1500/2217 | Loss 4.11250856413514\n",
      "Epoch 4 | Batch 1600/2217 | Loss 4.114479774538239\n",
      "Epoch 4 | Batch 1700/2217 | Loss 4.114794626297634\n",
      "Epoch 4 | Batch 1800/2217 | Loss 4.116245372635599\n",
      "Epoch 4 | Batch 1900/2217 | Loss 4.117890495804973\n",
      "Epoch 4 | Batch 2000/2217 | Loss 4.119452884350938\n",
      "Epoch 4 | Batch 2100/2217 | Loss 4.119736490222399\n",
      "Epoch 4 | Batch 2200/2217 | Loss 4.1201649162781235\n",
      "Epoch 5 | Batch 0/2217 | Loss 4.067425727844238\n",
      "Epoch 5 | Batch 100/2217 | Loss 4.028138972745083\n",
      "Epoch 5 | Batch 200/2217 | Loss 4.02962196169801\n",
      "Epoch 5 | Batch 300/2217 | Loss 4.03094994823798\n",
      "Epoch 5 | Batch 400/2217 | Loss 4.0306264908236455\n",
      "Epoch 5 | Batch 500/2217 | Loss 4.0358174156523985\n",
      "Epoch 5 | Batch 600/2217 | Loss 4.040126537126233\n",
      "Epoch 5 | Batch 700/2217 | Loss 4.043012391483563\n",
      "Epoch 5 | Batch 800/2217 | Loss 4.042647495400742\n",
      "Epoch 5 | Batch 900/2217 | Loss 4.043685303941022\n",
      "Epoch 5 | Batch 1000/2217 | Loss 4.0457056685761135\n",
      "Epoch 5 | Batch 1100/2217 | Loss 4.047631709387257\n",
      "Epoch 5 | Batch 1200/2217 | Loss 4.050079014974272\n",
      "Epoch 5 | Batch 1300/2217 | Loss 4.052103700865057\n",
      "Epoch 5 | Batch 1400/2217 | Loss 4.054047113652743\n",
      "Epoch 5 | Batch 1500/2217 | Loss 4.055447517435683\n",
      "Epoch 5 | Batch 1600/2217 | Loss 4.057281436806988\n",
      "Epoch 5 | Batch 1700/2217 | Loss 4.058548608714871\n",
      "Epoch 5 | Batch 1800/2217 | Loss 4.059323258958612\n",
      "Epoch 5 | Batch 1900/2217 | Loss 4.059643925270992\n",
      "Epoch 5 | Batch 2000/2217 | Loss 4.06117348454107\n",
      "Epoch 5 | Batch 2100/2217 | Loss 4.062316428361082\n",
      "Epoch 5 | Batch 2200/2217 | Loss 4.0631653443622024\n",
      "Epoch 6 | Batch 0/2217 | Loss 3.92167329788208\n",
      "Epoch 6 | Batch 100/2217 | Loss 3.9573716220289175\n",
      "Epoch 6 | Batch 200/2217 | Loss 3.965387822383672\n",
      "Epoch 6 | Batch 300/2217 | Loss 3.981497968154096\n",
      "Epoch 6 | Batch 400/2217 | Loss 3.986214715049154\n",
      "Epoch 6 | Batch 500/2217 | Loss 3.9892447413560634\n",
      "Epoch 6 | Batch 600/2217 | Loss 3.9912674764230127\n",
      "Epoch 6 | Batch 700/2217 | Loss 3.992805440823804\n",
      "Epoch 6 | Batch 800/2217 | Loss 3.9957356402341198\n",
      "Epoch 6 | Batch 900/2217 | Loss 3.995747092825988\n",
      "Epoch 6 | Batch 1000/2217 | Loss 3.9974690193420166\n",
      "Epoch 6 | Batch 1100/2217 | Loss 4.000170282187189\n",
      "Epoch 6 | Batch 1200/2217 | Loss 4.002963832971158\n",
      "Epoch 6 | Batch 1300/2217 | Loss 4.00540843731985\n",
      "Epoch 6 | Batch 1400/2217 | Loss 4.006602052107272\n",
      "Epoch 6 | Batch 1500/2217 | Loss 4.007761417588418\n",
      "Epoch 6 | Batch 1600/2217 | Loss 4.0077918359147695\n",
      "Epoch 6 | Batch 1700/2217 | Loss 4.00971445330867\n",
      "Epoch 6 | Batch 1800/2217 | Loss 4.011771681704566\n",
      "Epoch 6 | Batch 1900/2217 | Loss 4.012445648615765\n",
      "Epoch 6 | Batch 2000/2217 | Loss 4.01380600636152\n",
      "Epoch 6 | Batch 2100/2217 | Loss 4.014777019896546\n",
      "Epoch 6 | Batch 2200/2217 | Loss 4.015703831624573\n",
      "Epoch 7 | Batch 0/2217 | Loss 3.8997976779937744\n",
      "Epoch 7 | Batch 100/2217 | Loss 3.9263056434027037\n",
      "Epoch 7 | Batch 200/2217 | Loss 3.935572263613269\n",
      "Epoch 7 | Batch 300/2217 | Loss 3.9376192084974626\n",
      "Epoch 7 | Batch 400/2217 | Loss 3.938243021096969\n",
      "Epoch 7 | Batch 500/2217 | Loss 3.9423669293493093\n",
      "Epoch 7 | Batch 600/2217 | Loss 3.9425611726059495\n",
      "Epoch 7 | Batch 700/2217 | Loss 3.944320704559457\n",
      "Epoch 7 | Batch 800/2217 | Loss 3.9463489761066795\n",
      "Epoch 7 | Batch 900/2217 | Loss 3.9472579625285293\n",
      "Epoch 7 | Batch 1000/2217 | Loss 3.9507597078691115\n",
      "Epoch 7 | Batch 1100/2217 | Loss 3.9531105316085884\n",
      "Epoch 7 | Batch 1200/2217 | Loss 3.955217125413817\n",
      "Epoch 7 | Batch 1300/2217 | Loss 3.9572169012880436\n",
      "Epoch 7 | Batch 1400/2217 | Loss 3.958942097821123\n",
      "Epoch 7 | Batch 1500/2217 | Loss 3.960332511028872\n",
      "Epoch 7 | Batch 1600/2217 | Loss 3.961931793187872\n",
      "Epoch 7 | Batch 1700/2217 | Loss 3.9634985308448125\n",
      "Epoch 7 | Batch 1800/2217 | Loss 3.966327976743623\n",
      "Epoch 7 | Batch 1900/2217 | Loss 3.967708273601181\n",
      "Epoch 7 | Batch 2000/2217 | Loss 3.9696303477947383\n",
      "Epoch 7 | Batch 2100/2217 | Loss 3.9715753146320907\n",
      "Epoch 7 | Batch 2200/2217 | Loss 3.9730473459443956\n",
      "Epoch 8 | Batch 0/2217 | Loss 3.8417444229125977\n",
      "Epoch 8 | Batch 100/2217 | Loss 3.8640074611890434\n",
      "Epoch 8 | Batch 200/2217 | Loss 3.875492001054299\n",
      "Epoch 8 | Batch 300/2217 | Loss 3.882325592231117\n",
      "Epoch 8 | Batch 400/2217 | Loss 3.888655787988791\n",
      "Epoch 8 | Batch 500/2217 | Loss 3.8928114149622814\n",
      "Epoch 8 | Batch 600/2217 | Loss 3.8981724824762582\n",
      "Epoch 8 | Batch 700/2217 | Loss 3.9006649107803804\n",
      "Epoch 8 | Batch 800/2217 | Loss 3.9049640338816745\n",
      "Epoch 8 | Batch 900/2217 | Loss 3.9086958496737294\n",
      "Epoch 8 | Batch 1000/2217 | Loss 3.9111785393256646\n",
      "Epoch 8 | Batch 1100/2217 | Loss 3.9122929200597722\n",
      "Epoch 8 | Batch 1200/2217 | Loss 3.9161261583148788\n",
      "Epoch 8 | Batch 1300/2217 | Loss 3.9183299068667905\n",
      "Epoch 8 | Batch 1400/2217 | Loss 3.919554990500233\n",
      "Epoch 8 | Batch 1500/2217 | Loss 3.9203101891346726\n",
      "Epoch 8 | Batch 1600/2217 | Loss 3.9230060949688923\n",
      "Epoch 8 | Batch 1700/2217 | Loss 3.9245626742807573\n",
      "Epoch 8 | Batch 1800/2217 | Loss 3.9259201276970863\n",
      "Epoch 8 | Batch 1900/2217 | Loss 3.928574108814077\n",
      "Epoch 8 | Batch 2000/2217 | Loss 3.9303155083587202\n",
      "Epoch 8 | Batch 2100/2217 | Loss 3.9330359118033567\n",
      "Epoch 8 | Batch 2200/2217 | Loss 3.9344172542715876\n",
      "Epoch 9 | Batch 0/2217 | Loss 3.7696211338043213\n",
      "Epoch 9 | Batch 100/2217 | Loss 3.8445459427219806\n",
      "Epoch 9 | Batch 200/2217 | Loss 3.8535891302782503\n",
      "Epoch 9 | Batch 300/2217 | Loss 3.855046646143511\n",
      "Epoch 9 | Batch 400/2217 | Loss 3.8560705357358938\n",
      "Epoch 9 | Batch 500/2217 | Loss 3.860320260186871\n",
      "Epoch 9 | Batch 600/2217 | Loss 3.8613334963603343\n",
      "Epoch 9 | Batch 700/2217 | Loss 3.8654352727528134\n",
      "Epoch 9 | Batch 800/2217 | Loss 3.867296365911743\n",
      "Epoch 9 | Batch 900/2217 | Loss 3.8710083471948113\n",
      "Epoch 9 | Batch 1000/2217 | Loss 3.873439248625215\n",
      "Epoch 9 | Batch 1100/2217 | Loss 3.8761808644848235\n",
      "Epoch 9 | Batch 1200/2217 | Loss 3.8790757971738996\n",
      "Epoch 9 | Batch 1300/2217 | Loss 3.8799691324871746\n",
      "Epoch 9 | Batch 1400/2217 | Loss 3.8821376032015835\n",
      "Epoch 9 | Batch 1500/2217 | Loss 3.8847089314127192\n",
      "Epoch 9 | Batch 1600/2217 | Loss 3.887933701295394\n",
      "Epoch 9 | Batch 1700/2217 | Loss 3.890437027904863\n",
      "Epoch 9 | Batch 1800/2217 | Loss 3.892214157924197\n",
      "Epoch 9 | Batch 1900/2217 | Loss 3.8941119593861604\n",
      "Epoch 9 | Batch 2000/2217 | Loss 3.896760531272488\n",
      "Epoch 9 | Batch 2100/2217 | Loss 3.898651454177486\n",
      "Epoch 9 | Batch 2200/2217 | Loss 3.9002577191534393\n",
      "Epoch 10 | Batch 0/2217 | Loss 3.8531837463378906\n",
      "Epoch 10 | Batch 100/2217 | Loss 3.808265447616577\n",
      "Epoch 10 | Batch 200/2217 | Loss 3.8100689347110577\n",
      "Epoch 10 | Batch 300/2217 | Loss 3.8163841269737064\n",
      "Epoch 10 | Batch 400/2217 | Loss 3.8202970408442014\n",
      "Epoch 10 | Batch 500/2217 | Loss 3.824556264572753\n",
      "Epoch 10 | Batch 600/2217 | Loss 3.8282433242448755\n",
      "Epoch 10 | Batch 700/2217 | Loss 3.831641563505317\n",
      "Epoch 10 | Batch 800/2217 | Loss 3.8348862029491144\n",
      "Epoch 10 | Batch 900/2217 | Loss 3.8384471582651933\n",
      "Epoch 10 | Batch 1000/2217 | Loss 3.8400622014399177\n",
      "Epoch 10 | Batch 1100/2217 | Loss 3.842984128712958\n",
      "Epoch 10 | Batch 1200/2217 | Loss 3.8455982279717973\n",
      "Epoch 10 | Batch 1300/2217 | Loss 3.849820505005868\n",
      "Epoch 10 | Batch 1400/2217 | Loss 3.852379105075779\n",
      "Epoch 10 | Batch 1500/2217 | Loss 3.8538387773197385\n",
      "Epoch 10 | Batch 1600/2217 | Loss 3.8553528300231132\n",
      "Epoch 10 | Batch 1700/2217 | Loss 3.8572206854610007\n",
      "Epoch 10 | Batch 1800/2217 | Loss 3.8593235849870835\n",
      "Epoch 10 | Batch 1900/2217 | Loss 3.8615715461050693\n",
      "Epoch 10 | Batch 2000/2217 | Loss 3.863179164549996\n",
      "Epoch 10 | Batch 2100/2217 | Loss 3.8654309309760144\n",
      "Epoch 10 | Batch 2200/2217 | Loss 3.8672788151823787\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, 'checkpoint/checkpoint_' + str(epoch) + '.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoint/checkpoint_9.tar')\n",
    "model = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m question \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(enc_question)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m question_mask \u001b[39m=\u001b[39m (question\u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m sentence \u001b[39m=\u001b[39m evaluate(model, question, question_mask, \u001b[39mint\u001b[39;49m(max_len), word_map)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(sentence)\n",
      "Cell \u001b[1;32mIn[31], line 35\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(transformer, question, question_mask, max_len, word_map)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m next_word \u001b[39m==\u001b[39m word_map[\u001b[39m'\u001b[39m\u001b[39m<end>\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     34\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m words \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([words, torch\u001b[39m.\u001b[39mLongTensor([[next_word]]\u001b[39m.\u001b[39;49mto(device))], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#(1, step+2)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39m#(1,5) -> (5)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m words \u001b[39m=\u001b[39m words\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# (1, step+1) -> (step+1)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \")\n",
    "    if question == 'quit':\n",
    "        break\n",
    "    max_len = input(\"Enter Max Words to be generated: \")\n",
    "    enc_question = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    question = torch.LongTensor(enc_question).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
    "    sentence = evaluate(model, question, question_mask, int(max_len), word_map)\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
